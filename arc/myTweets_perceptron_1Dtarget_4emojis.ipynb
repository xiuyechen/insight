{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_names = ['üòç','üò°'] #['üòç','üòú','üò≠','üò°']\n",
    "target_names = ['üòç','üòÄ','ü§î','üôÑ']\n",
    "\n",
    "numTweets = 5000\n",
    "data = []\n",
    "filenames = []\n",
    "for keyword in target_names:\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k_0921/\"+'outfile'+keyword)\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        itemlist = pickle.load(fp)\n",
    "        L = []\n",
    "        for tweet in itemlist:\n",
    "            # take out all emojis from input tweets\n",
    "            emoji_list = extract_emojis(tweet)\n",
    "            text = re.sub(\"|\".join(emoji_list), \"\", tweet)\n",
    "            L.append(text)\n",
    "        data = data + L\n",
    "        filenames = filenames + [fullfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEmojis = len(target_names)\n",
    "arr = []\n",
    "for i in range(numEmojis):\n",
    "    arr.extend([i] * numTweets)\n",
    "target = np.array(arr, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(re.findall(r'[@]\\S*', itemlist[:10])) # Twitter user names\n",
    "#print(re.findall(r'[RT]\\S*', str)) # \"RT\"\n",
    "#print(re.findall(r'[http]\\S*', str)) # \"RT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Tweet dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is like a struct\n",
    "class tweet_train:\n",
    "    pass\n",
    "\n",
    "T = tweet_train()\n",
    "T.target_names = target_names\n",
    "T.data = data\n",
    "T.filenames = filenames\n",
    "T.target = target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòç\n"
     ]
    }
   ],
   "source": [
    "print(T.target_names[T.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing text with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pre-trained word2vec model (from Twitter data, glove.twitter.27B.zip)\n",
    "# fdir = fullfile = os.path.expanduser(\"~/Documents/glovetwitter27B/glovetwitter27B25d.txt\")\n",
    "\n",
    "# with open(fdir, \"rb\") as lines:\n",
    "#     w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "#            for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                            max_features=100,\n",
    "                            stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(T.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From occurrences to frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, T.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    T.data, T.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char',\n",
    "                             use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "# TASK: Build a vectorizer / classifier pipeline using the previous analyzer\n",
    "# the pipeline instance should stored in a variable named clf\n",
    "clf = Pipeline([\n",
    "    ('vec', vectorizer),\n",
    "    ('clf', Perceptron(tol=1e-3)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ..._jobs=1, penalty=None, random_state=0,\n",
       "      shuffle=True, tol=0.001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: Fit the pipeline on the training set\n",
    "clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          üòç       0.73      0.69      0.71      2520\n",
      "          üòÄ       0.60      0.77      0.68      2474\n",
      "          ü§î       0.62      0.83      0.71      2475\n",
      "          üôÑ       0.79      0.35      0.48      2531\n",
      "\n",
      "avg / total       0.69      0.66      0.64     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=T.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n",
    "\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall) \n",
    "https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/metrics/classification.py#L1363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1740  466  239   75]\n",
      " [ 222 1917  262   73]\n",
      " [ 107  244 2042   82]\n",
      " [ 323  570  755  883]]\n"
     ]
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm, cmap=plt.cm.jet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    u'lovely definition: 1. pleasant or enjoyable: 2. beautiful:',\n",
    "    u'Hate speech is a communication that carries no meaning other than the expression of hatred for some group',\n",
    "    u'amazing wow love this!!!',\n",
    "]\n",
    "predicted = clf.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0]\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "strList = ['Biological life cycle - WikipediaIn biology, a biological life cycle is a series of changes in form that an organism \\nundergoes, returning to the starting state. \"The concept is closely related to those\\n\\xa0...',\n",
    " 'Life Itself (2018 film) - WikipediaLife Itself is a 2018 American drama film written and directed by Dan Fogelman. It \\nstars Oscar Isaac, Olivia Wilde, Mandy Patinkin, Olivia Cooke, Laia Costa,\\xa0...',\n",
    " 'Everyday life - WikipediaEveryday life, daily life or routine life comprises the ways in which people \\ntypically act, think, and feel on a daily basis. Everyday life may be described as\\xa0...',\n",
    " 'List of life sciences - WikipediaThe life sciences or biological sciences comprise the branches of science that \\ninvolve the scientific study of life and organisms ‚Äì such as microorganisms, \\nplants,\\xa0...',\n",
    " 'Quality of life - WikipediaQuality of life (QOL) is the general well-being of individuals and societies, \\noutlining negative and positive features of life. It observes life satisfaction, \\nincluding\\xa0...',\n",
    " 'Life (The Cardigans album) - WikipediaLife is the second studio album of The Cardigans. It was released worldwide in \\n1995 and was an international success, especially in Japan, where it achieved\\xa0...',\n",
    " 'The Book of Life (Harkness novel) - WikipediaThe Book of Life is a 2014 fantasy novel by American scholar Deborah Harkness, \\nthe third book in the All Souls trilogy. As the sequel to the 2012 bestseller,\\xa0...',\n",
    " 'Life imprisonment - WikipediaLife imprisonment is any sentence of imprisonment for a crime under which \\nconvicted persons are to remain in prison either for the rest of their natural life or \\nuntil\\xa0...',\n",
    " 'Life (Keith Richards) - WikipediaLife is a memoir by the Rolling Stones guitarist Keith Richards, written with the \\nassistance of journalist James Fox. Published in October 2010, in hardback, \\naudio\\xa0...',\n",
    " 'The Game of Life - WikipediaThe Game of Life, also known simply as Life, is a board game originally created \\nin 1860 by Milton Bradley, as The Checkered Game of Life. The Game of Life\\xa0...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 2 1 1 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(strList)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight_side/\"+'clf_0925.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(clf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 283,
   "position": {
    "height": "40px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "326px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
