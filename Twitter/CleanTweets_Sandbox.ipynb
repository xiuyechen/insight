{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            \n",
    "            text = re.sub(\"|\".join(word), \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜˜', 'ğŸ˜—', 'ğŸ˜™', 'ğŸ˜š', 'â˜ºï¸', 'ğŸ™‚', 'ğŸ¤—', 'ğŸ¤”', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜¶', 'ğŸ™„', 'ğŸ˜', 'ğŸ˜£', 'ğŸ˜¥', 'ğŸ˜®', 'ğŸ¤', 'ğŸ˜¯', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ˜´', 'ğŸ˜Œ', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤¤', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ™ƒ', 'ğŸ¤‘', 'ğŸ˜²', 'â˜¹ï¸', 'ğŸ™', 'ğŸ˜–', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜¤', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜¬', 'ğŸ˜°', 'ğŸ˜±', 'ğŸ˜³', 'ğŸ˜µ', 'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ˜·', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ¤¢', 'ğŸ¤§', 'ğŸ˜‡', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¥', 'ğŸ¤“']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# load emoji list\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Emoji/\"+'mySmileys.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    emoji_list = pickle.load(fp)\n",
    "print(emoji_list)\n",
    "print(len(emoji_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing: word-to-vec with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ğŸ˜€': 0, 'ğŸ˜': 1, 'ğŸ˜‚': 2, 'ğŸ¤£': 3, 'ğŸ˜ƒ': 4, 'ğŸ˜„': 5, 'ğŸ˜…': 6, 'ğŸ˜†': 7, 'ğŸ˜‰': 8, 'ğŸ˜Š': 9, 'ğŸ˜‹': 10, 'ğŸ˜': 11, 'ğŸ˜': 12, 'ğŸ˜˜': 13, 'ğŸ˜—': 14, 'ğŸ˜™': 15, 'ğŸ˜š': 16, 'â˜ºï¸': 17, 'ğŸ™‚': 18, 'ğŸ¤—': 19, 'ğŸ¤”': 20, 'ğŸ˜': 21, 'ğŸ˜‘': 22, 'ğŸ˜¶': 23, 'ğŸ™„': 24, 'ğŸ˜': 25, 'ğŸ˜£': 26, 'ğŸ˜¥': 27, 'ğŸ˜®': 28, 'ğŸ¤': 29, 'ğŸ˜¯': 30, 'ğŸ˜ª': 31, 'ğŸ˜«': 32, 'ğŸ˜´': 33, 'ğŸ˜Œ': 34, 'ğŸ˜›': 35, 'ğŸ˜œ': 36, 'ğŸ˜': 37, 'ğŸ¤¤': 38, 'ğŸ˜’': 39, 'ğŸ˜“': 40, 'ğŸ˜”': 41, 'ğŸ˜•': 42, 'ğŸ™ƒ': 43, 'ğŸ¤‘': 44, 'ğŸ˜²': 45, 'â˜¹ï¸': 46, 'ğŸ™': 47, 'ğŸ˜–': 48, 'ğŸ˜': 49, 'ğŸ˜Ÿ': 50, 'ğŸ˜¤': 51, 'ğŸ˜¢': 52, 'ğŸ˜­': 53, 'ğŸ˜¦': 54, 'ğŸ˜§': 55, 'ğŸ˜¨': 56, 'ğŸ˜©': 57, 'ğŸ˜¬': 58, 'ğŸ˜°': 59, 'ğŸ˜±': 60, 'ğŸ˜³': 61, 'ğŸ˜µ': 62, 'ğŸ˜¡': 63, 'ğŸ˜ ': 64, 'ğŸ˜·': 65, 'ğŸ¤’': 66, 'ğŸ¤•': 67, 'ğŸ¤¢': 68, 'ğŸ¤§': 69, 'ğŸ˜‡': 70, 'ğŸ¤ ': 71, 'ğŸ¤¡': 72, 'ğŸ¤¥': 73, 'ğŸ¤“': 74}\n"
     ]
    }
   ],
   "source": [
    "N = []\n",
    "for ii in range(75):\n",
    "    N.append(ii)\n",
    "Edict = dict(zip(emoji_list, N))\n",
    "print(Edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     and other words! '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize_tweets(tweet):\n",
    "    # delete twitter-specific tags\n",
    "    b1 = re.findall(r'@\\S*', tweet) # Twitter user names\n",
    "    b2 = re.findall(r'RT\\S*', tweet) # \"RT\"\n",
    "    b3 = re.findall(r'http\\S*', tweet) # links\n",
    "\n",
    "    c = tweet #list(tweet.split())\n",
    "    for b in b1:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b2:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b3:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    return c\n",
    "    \n",
    "tweet = \"RT @aa RT @bbb https:__ and other words! httpsaa\" #RawT[-1]\n",
    "sanitize_tweets(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word',# min_df=3, ngram_range=(1, 3)\n",
    "                             max_features = 2000, #stop_words='english',\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\")\n",
    "\n",
    "# https://stackoverflow.com/questions/39254134/how-to-preserve-punctuation-marks-in-scikit-learn-text-countvectorizer-or-tfidfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+'ğŸ˜'+'.p') # 'ğŸ˜±'+'.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    Tweets = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜€\n",
      "ğŸ˜\n",
      "ğŸ˜‚\n",
      "ğŸ¤£\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜…\n",
      "ğŸ˜†\n",
      "ğŸ˜‰\n",
      "ğŸ˜Š\n",
      "ğŸ˜‹\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜˜\n",
      "ğŸ˜—\n",
      "ğŸ˜™\n",
      "ğŸ˜š\n",
      "â˜ºï¸\n",
      "ğŸ™‚\n",
      "ğŸ¤—\n",
      "ğŸ¤”\n",
      "ğŸ˜\n",
      "ğŸ˜‘\n",
      "ğŸ˜¶\n",
      "ğŸ™„\n",
      "ğŸ˜\n",
      "ğŸ˜£\n",
      "ğŸ˜¥\n",
      "ğŸ˜®\n",
      "ğŸ¤\n",
      "ğŸ˜¯\n",
      "ğŸ˜ª\n",
      "ğŸ˜«\n",
      "ğŸ˜´\n",
      "ğŸ˜Œ\n",
      "ğŸ˜›\n",
      "ğŸ˜œ\n",
      "ğŸ˜\n",
      "ğŸ¤¤\n",
      "ğŸ˜’\n",
      "ğŸ˜“\n",
      "ğŸ˜”\n",
      "ğŸ˜•\n",
      "ğŸ™ƒ\n",
      "ğŸ¤‘\n",
      "ğŸ˜²\n",
      "â˜¹ï¸\n",
      "ğŸ™\n",
      "ğŸ˜–\n",
      "ğŸ˜\n",
      "ğŸ˜Ÿ\n",
      "ğŸ˜¤\n",
      "ğŸ˜¢\n",
      "ğŸ˜­\n",
      "ğŸ˜¦\n",
      "ğŸ˜§\n",
      "ğŸ˜¨\n",
      "ğŸ˜©\n",
      "ğŸ˜¬\n",
      "ğŸ˜°\n",
      "ğŸ˜±\n",
      "ğŸ˜³\n",
      "ğŸ˜µ\n",
      "ğŸ˜¡\n",
      "ğŸ˜ \n",
      "ğŸ˜·\n",
      "ğŸ¤’\n",
      "ğŸ¤•\n",
      "ğŸ¤¢\n",
      "ğŸ¤§\n",
      "ğŸ˜‡\n",
      "ğŸ¤ \n",
      "ğŸ¤¡\n",
      "ğŸ¤¥\n",
      "ğŸ¤“\n"
     ]
    }
   ],
   "source": [
    "# preprocessing with removing repeat tweets\n",
    "# IN\n",
    "target_names = emoji_list # ['ğŸ˜','ğŸ˜¡'] # \n",
    "\n",
    "# OUT\n",
    "rawdata = []\n",
    "data = []\n",
    "raw_target = []\n",
    "Len = []\n",
    "filenames = []\n",
    "target_arr = np.zeros((5000*75, 75))\n",
    "search_emoji = []\n",
    "\n",
    "# main loop\n",
    "ii = 0\n",
    "for keyword in target_names:\n",
    "    print(keyword)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets = pickle.load(fp)\n",
    "\n",
    "        # remove repeat tweets\n",
    "        myset = set(Tweets)\n",
    "        Tweets_unique = list(myset)\n",
    "\n",
    "        Text = []\n",
    "        RawText = []\n",
    "        E = []\n",
    "        for tweet in Tweets_unique:\n",
    "            # remove tweets with more than one smiley\n",
    "            emojis = extract_emojis(tweet)\n",
    "            a = set(emojis) & set(emoji_list)\n",
    "            a2 = list(a)\n",
    "            if len(a2)==1 and a2[0]==keyword: # else: deal with later if doing multi-D target\n",
    "                \n",
    "                # save raw tweet\n",
    "                RawText.append(tweet)\n",
    "                # take out all emojis from input tweets\n",
    "                emojis = extract_emojis(tweet)\n",
    "                E.append(emojis)\n",
    "\n",
    "#                 # save emojis into target array \n",
    "#                 for x in emojis:\n",
    "#                     if x in emoji_list:\n",
    "#                         ix = Edict[x]\n",
    "#                         target_arr[ii,ix] = 1\n",
    "\n",
    "                # save sanitized text\n",
    "                text = sanitize_tweets(tweet)\n",
    "                text = delete_emojis(text)\n",
    "                Text.append(text)\n",
    "                ii = ii+1\n",
    "            \n",
    "        data = data + Text\n",
    "        rawdata = rawdata + RawText\n",
    "        raw_target = raw_target + E\n",
    "        filenames = filenames + [fullfile]\n",
    "        search_emoji = search_emoji + [keyword]\n",
    "        Len.append(len(Text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ˜', 'â¤ï¸', 'ğŸ¶', 'ğŸ¤', 'ğŸ‚', 'ğŸ‰', 'ğŸ', 'ğŸˆ', 'ğŸ˜', 'ğŸŒŸ', 'ğŸ’«']\n",
      "{'ğŸ˜'}\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tweet = \"ğŸ˜ â¤ï¸ğŸ¶ğŸ¤ğŸ‚ğŸ‰ğŸğŸˆğŸ˜ğŸŒŸğŸ’« shsha;lsdij \"\n",
    "emojis = extract_emojis(tweet)\n",
    "print(emojis)\n",
    "a = set(emojis) & set(emoji_list)\n",
    "a2 = list(a)\n",
    "print(a)\n",
    "print(len(a2)==1 and a2[0]=='ğŸ˜')\n",
    "print(a2[0]=='ğŸ˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "['â¤ï¸', 'ğŸ¶', 'ğŸ¤', 'ğŸ‚', 'ğŸ‰', 'ğŸ', 'ğŸˆ', 'ğŸ˜', 'ğŸŒŸ', 'ğŸ’«', 'ğŸ‡']\n",
      "['ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['â™¥ï¸', 'â™¥ï¸', 'â™¥ï¸', 'ğŸ˜˜', 'ğŸ˜˜', 'ğŸ˜', 'ğŸ˜‡']\n",
      "['ğŸ˜', 'ğŸ’–', 'ğŸ’–']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±', 'ğŸ˜±']\n",
      "['ğŸ’ƒ', 'ğŸ’ƒ', 'ğŸ’ƒ', 'ğŸ˜˜', 'ğŸ˜', 'ğŸ’•']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'â¤ï¸']\n",
      "['ğŸ‘', 'ğŸ‘', 'ğŸ‘', 'ğŸ˜', 'ğŸ‘¶', 'ğŸ’¦', 'ğŸ’–']\n",
      "['ğŸ˜', 'ğŸ˜­', 'ğŸ’“']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ’œ']\n",
      "['ğŸ˜']\n",
      "['ğŸ‰', 'ğŸ‰', 'ğŸ', 'ğŸ', 'ğŸ’°', 'âœ…', 'âœ…']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜˜']\n",
      "['ğŸ™Œ', 'ğŸ˜', 'ğŸ‘']\n",
      "['ğŸ˜©', 'ğŸ˜', 'ğŸ’¯', 'ğŸ”¥', 'ğŸ”¥']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜»', 'ğŸ˜»', 'ğŸ˜»']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ‘‘', 'ğŸ‘‘']\n",
      "[]\n",
      "['ğŸ˜']\n",
      "[]\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ¥€']\n",
      "['ğŸ˜', 'ğŸ˜˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜­', 'ğŸ˜­', 'ğŸ˜­', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜‡', 'ğŸ˜­', 'ğŸ˜', 'ğŸ˜˜']\n",
      "['ğŸ’š', 'ğŸ˜Š']\n",
      "['ğŸ˜', 'ğŸ¶', 'ğŸ¶', 'ğŸ¶', 'ğŸ¶', 'ğŸ¶', 'ğŸ¶', 'ğŸ¶', 'ğŸ¶', 'ğŸ¶']\n",
      "['ğŸ˜']\n",
      "[]\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ”¥', 'â¤ï¸']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜‡', 'ğŸ˜', 'ğŸ˜˜']\n",
      "['ğŸ’•']\n",
      "['ğŸ˜˜', 'ğŸ˜˜', 'ğŸ˜']\n",
      "[]\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ”¥', 'ğŸ”¥', 'ğŸ”¥']\n",
      "[]\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ’¦', 'ğŸ’¦', 'ğŸ˜']\n",
      "[]\n",
      "['ğŸ˜']\n",
      "['ğŸ˜›', 'ğŸ˜', 'ğŸ™Œ', 'ğŸ™‹', 'ğŸ™†']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜˜', 'ğŸ˜˜', 'ğŸ˜¬', 'ğŸ˜¬']\n",
      "['ğŸ˜­', 'ğŸ˜­', 'ğŸ˜­', 'ğŸ˜‚', 'ğŸ˜­', 'ğŸ˜‚', 'ğŸ˜‚', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ’ƒ', 'ğŸ’ƒ', 'ğŸ’ƒ']\n",
      "['ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ’™', 'ğŸ’¦', 'ğŸ’œ', 'ğŸ’™', 'ğŸ’Ÿ', 'ğŸ’™', 'ğŸ’œ', 'ğŸ’¦', 'ğŸ’™']\n",
      "['\\U0001f996', '\\U0001f995', 'ğŸ˜', 'ğŸ‘']\n",
      "[]\n",
      "['ğŸ±', 'ğŸ˜', 'ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜®', 'ğŸ˜']\n",
      "['ğŸ’–', 'ğŸ˜']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜¤']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜­', 'ğŸ˜­', 'ğŸ˜­']\n",
      "['ğŸ˜']\n",
      "['ğŸ˜', 'ğŸ˜Š', 'ğŸ¶']\n"
     ]
    }
   ],
   "source": [
    "mylist = Tweets[:100]\n",
    "myset = set(mylist)\n",
    "mynewlist = list(myset)\n",
    "print(len(mynewlist))\n",
    "for tweet in mynewlist:\n",
    "    emojis = extract_emojis(tweet)\n",
    "    print(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @WORLDMUSICAWARD: Happy 26th Birthday to #EXO's very handsome and talented #CHEN! #HappyChenDay! @weareoneEXO\n",
      "â¤ï¸ğŸ‡°ğŸ‡·ğŸ¶ğŸ¤ğŸ‚ğŸ‰ğŸğŸˆğŸ˜ğŸŒŸğŸ’«ğŸ‡  \n",
      "https://tâ€¦â¤ï¸ğŸ¶ğŸ¤ğŸ‚ğŸ‰ğŸğŸˆğŸ˜ğŸŒŸğŸ’«ğŸ‡\n",
      "----------------\n",
      "RT @wx8: The buoy cam tonight in South Haven 10/10 ğŸ˜ğŸ˜ https://t.co/WYSrUmNYcIğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @SundayChants: Volleyed rabona goal from outside the box. Class. ğŸ˜ https://t.co/zFerMsOGpYğŸ˜\n",
      "----------------\n",
      "RT @RyderCupEurope: In case you weren't already excited ğŸ˜\n",
      "\n",
      "#RyderCup #TeamEurope https://t.co/7YerfQROe7ğŸ˜\n",
      "----------------\n",
      "@yejiapshai You're welcome pooo ğŸ˜ğŸ’–ğŸ’–ğŸ˜ğŸ’–ğŸ’–\n",
      "----------------\n",
      "@takethat Love it ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜\n",
      "----------------\n",
      "@ColorsTV @sharmarashmi20 @VivianDsena01 @RubiDilaik Precap: oh noğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±..wht if Agni tells Jolly tht Khushi iâ€¦ https://t.co/XkA1mLZWQfğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±\n",
      "----------------\n",
      "Kate Winslet ğŸ˜ğŸ˜\n",
      "----------------\n",
      "Congratulations @AssaultSethuuu for 1K Viswasamana Followers ğŸ˜ğŸ˜ğŸ˜\n",
      "#Viswasam â¤ï¸ğŸ˜ğŸ˜ğŸ˜â¤ï¸\n",
      "----------------\n",
      "RT @Ryan_CaoDat: The baby( about 1 year old) can swim well!ğŸ‘ğŸ‘ğŸ‘ğŸ˜ğŸ‘¶ğŸ’¦ğŸ’– https://t.co/iJE4z1VZTağŸ‘ğŸ‘ğŸ‘ğŸ˜ğŸ‘¶ğŸ’¦ğŸ’–\n",
      "----------------\n",
      "My boy so cute ğŸ˜.ğŸ˜\n",
      "----------------\n",
      "RT @daydreamer7667: GOT7's spotify header ğŸ˜ğŸ’œ \n",
      " https://t.co/H1IVkuMmKd\n",
      "#GOT7 #ê°“ì„¸ë¸ #PresentYOU #Lullaby @GOT7Official https://t.co/cigROK60DmğŸ˜ğŸ’œ\n",
      "----------------\n",
      "@KodakBlack1k I love you ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @oldiesclub: Meet Sybil ğŸ˜ Sybil is a 10+ year old Yorkie who has just arrived in Oldies Club care. Sybil will be under assessment for thâ€¦ğŸ˜\n",
      "----------------\n",
      "Yes please! ğŸ˜ https://t.co/a54975qdBOğŸ˜\n",
      "----------------\n",
      "RT @PinkBoutiqueUK: IT'S THAT TIME AGAIN! ğŸ™ŒğŸ˜ RT and FOLLOW for your chance to win a Â£100 voucher  ğŸ‘ #freebiefriday #competition #giveaway hâ€¦ğŸ™ŒğŸ˜ğŸ‘\n",
      "----------------\n",
      "@urjchaeyeon @PRODUCESIAR nunna dissy ğŸ˜ğŸ˜\n",
      "----------------\n",
      "@JordanPeele AHHHHHHHHHHH!! THIS IS SO FUCKING GREAT!! ğŸ˜ğŸ˜\n",
      "----------------\n",
      "@YOSHEROSE Then Iâ€™m gna go out to have my lunch ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @ChuBoi: Goal of my life from Sarr ğŸ˜ https://t.co/bDb3Fnrr9wğŸ˜\n",
      "----------------\n",
      "@radin0007 Yeesss! ğŸ˜ğŸ˜ğŸ˜ğŸ˜»ğŸ˜»ğŸ˜»ğŸ˜ğŸ˜ğŸ˜ğŸ˜»ğŸ˜»ğŸ˜»\n",
      "----------------\n",
      "@becslovesTT @HowardDonald Love him !!! ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @izzat_syafie: ï¼£ï½ï½ï¼‡ï½”ã€€ï¼´ï½ï½‹ï½…ã€€ï¼­ï½™ã€€ï¼¥ï½™ï½…ï½“ã€€ï¼¯ï½†ã€€ï¼¹ï½ï½• ğŸ˜ https://t.co/pscttayuydğŸ˜\n",
      "----------------\n",
      "@tweyj96 @GOT7Official I really like it too ğŸ˜ğŸ˜ when ever you think he coulnt look better he comes around the cornerâ€¦ https://t.co/EgmstNznM9ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @alyyy0469: i failed my physics test. ğŸ˜ https://t.co/pIBWt8r1jzğŸ˜\n",
      "----------------\n",
      "RT @The_BForce: A vision ğŸ˜ğŸ˜ğŸ‘‘ğŸ‘‘ https://t.co/HkLZTPSgXiğŸ˜ğŸ˜ğŸ‘‘ğŸ‘‘\n",
      "----------------\n",
      "RT @Animalzoo22: \"Cat vs catsnake\" &lt;- That floop in the end ğŸ˜ https://t.co/tR0pTncO7gğŸ˜\n",
      "----------------\n",
      "RT @Itsdogsvscats: Head rubsğŸ˜ https://t.co/0UForeHa9fğŸ˜\n",
      "----------------\n",
      "RT @meothmans: RT may be my sisterâ€™s husband is on your TL ğŸ˜ğŸ¥€ https://t.co/8ZkmekPG5DğŸ˜ğŸ¥€\n",
      "----------------\n",
      "@aleeintheskyy He is so pretty ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @Dinahscrown: DINAH IS REALLY BEAUTIFUL ğŸ˜\n",
      "\n",
      "#BottledUpOutNow https://t.co/ZaFM62MOZuğŸ˜\n",
      "----------------\n",
      "RT @iloveelephant26: There is NOTHING cuter than a baby elephant ğŸ˜  \n",
      " from @thesafaricottages  \n",
      "ğŸ˜#elephant #elephants #elephantlove #saveelâ€¦ğŸ˜ğŸ˜\n",
      "----------------\n",
      "@fixmyhrtddl @BigHitEnt 'Wolf Prince' JinğŸ˜ğŸ˜ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @kissyeollips: Chanyeol's Vogue Korea October 2018 Issue ğŸ’š\n",
      "Our Tommy Boy is not just a pretty face, but the star of the show ğŸ˜Š His tallâ€¦ğŸ’šğŸ˜Š\n",
      "----------------\n",
      "RT @BBCWorld: ğŸ˜ Meet the cutest and newest recruits for Chile's national police force ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ https://t.co/lLwVO7mB7ağŸ˜ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶\n",
      "----------------\n",
      "GOT7 IS LEADINGGG!!! ğŸ˜ #LULLABY #GOT7 #MCOUNTDOWN \n",
      "VOTE HERE: https://t.co/94NgQGh32V https://t.co/2DwSwhblD0ğŸ˜\n",
      "----------------\n",
      "The new iPhone update ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ”¥â¤ï¸ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ”¥â¤ï¸\n",
      "----------------\n",
      "RT @cel_antonio12: I think we can all agree that we are shook by those outfits. We want more ğŸ˜\n",
      "\n",
      "PS. The closeup on Jinyoung at ending is liâ€¦ğŸ˜\n",
      "----------------\n",
      "RT @NAgarwal8: Picture of the dayğŸ˜ https://t.co/F9CpHOMFUmğŸ˜\n",
      "----------------\n",
      "RT @YoohyeonWhat: Hee-jin\n",
      "(LOONA)\n",
      "Pencil\n",
      "---------------------\n",
      "I love her so much! She's my biasğŸ˜\n",
      "-----------------------------\n",
      "-\n",
      "-\n",
      "-\n",
      "-&gt; #hâ€¦ğŸ˜\n",
      "----------------\n",
      "@Marge_theBrave Awe... I can bring some and a bucket of salt. Elo Marge you have beautiful eyes. ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @omgitslolly: @RYESOREO Can yâ€™all just get married all ready ong goals ğŸ˜ğŸ˜\n",
      "----------------\n",
      "@fiah213 @_Twinndiesel An Andy Mineo review ğŸ˜ğŸ”¥ğŸ”¥ğŸ”¥ bars on barsğŸ˜ğŸ”¥ğŸ”¥ğŸ”¥\n",
      "----------------\n",
      "RT @markantardesign: 2021 @MercedesAMGF1 with a @LewisHamilton twist! ğŸ˜\n",
      "\n",
      "Follow me on Instagram for more: https://t.co/kNXv3ZNaMb https://tâ€¦ğŸ˜\n",
      "----------------\n",
      "So cute nd lovely picture of @Pvsindhu1 ğŸ˜ğŸ˜ğŸ˜ #childhood https://t.co/nvcXb0Zd6sğŸ˜ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @Uncle_Costy: twÉ› nsuo ne T bread&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @Montopia_0912: [HD] 20180916 Love Yourself Tour in Fort Worth\n",
      "\n",
      "Sweaty Joon ğŸ’¦ğŸ’¦ğŸ˜\n",
      "\n",
      " @BTS_twt #RM #ì•Œì—  #Namjoon #ë‚¨ì¤€ #ê¹€ë‚¨ì¤€ #BTSâ  â â #ë°©íƒ„ì†Œë…„ë‹¨ httpâ€¦ğŸ’¦ğŸ’¦ğŸ˜\n",
      "----------------\n",
      "You most definitely donâ€™t want to miss out on this! ğŸ˜ Get it now from Bou Melhem with #bitfood!\n",
      "#JouneyOfFood #Foodâ€¦ https://t.co/R7kqyWhNboğŸ˜\n",
      "----------------\n",
      "RT @QueenCazza92: @nutmegiwobi I LOVE YOU ğŸ˜ğŸ˜\n",
      "----------------\n",
      "The men of the Lee Singers ğŸ˜ especially after that performance in performance seminar! Best guys on campus!ğŸ˜\n",
      "----------------\n",
      "RT @ZVicentia: Everyone Knows I love @black_motion... This #JoyJoy song has me feeling ğŸ˜ğŸ˜ğŸ˜ğŸ’ƒğŸ’ƒğŸ’ƒ https://t.co/GlZbbsCHlIğŸ˜ğŸ˜ğŸ˜ğŸ’ƒğŸ’ƒğŸ’ƒ\n",
      "----------------\n",
      "RT @PestoChampion: Ultimate RecipeğŸ˜ Watch HERE &gt;&gt; https://t.co/65NAejJeO4  &lt;&lt; How to #Cook #IndianFood #NihariğŸ˜ #IndianFood #Video by #Londâ€¦ğŸ˜ğŸ˜\n",
      "----------------\n",
      "RT @TLeoni311: ğŸ˜Happy Thursday ğŸ˜\n",
      "      ğŸ’™ğŸ’¦ğŸ’œğŸ’™ğŸ’ŸğŸ’™ğŸ’œğŸ’¦ğŸ’™ https://t.co/Cv4CfRPZFvğŸ˜ğŸ˜ğŸ’™ğŸ’¦ğŸ’œğŸ’™ğŸ’ŸğŸ’™ğŸ’œğŸ’¦ğŸ’™\n",
      "----------------\n",
      "@Shop_at_NHM #freebiefriday roar Iâ€™d love this for my girls ğŸ¦– ğŸ¦• ğŸ˜ğŸ‘ğŸ¦–ğŸ¦•ğŸ˜ğŸ‘\n",
      "----------------\n",
      "RT @elysian9397: ğŸ± : jungkook!!!\n",
      "I love this team ğŸ˜ğŸ˜ https://t.co/C7BDSZwCxBğŸ±ğŸ˜ğŸ˜\n",
      "----------------\n",
      "WANT TO GET A FEEL FOR LE BOULEVARD BEFORE THE WEEKEND? ğŸ˜\n",
      "3 BEDROOMS, Â£1,800PCM - GROUVILLE\n",
      "https://t.co/nJ2Yz6aTIa https://t.co/nJ2Yz6aTIağŸ˜\n",
      "----------------\n",
      "RT @baekchannie: LOOK AT THE AMOUNT OF PEOPLE UGHHH!! \n",
      "SIDE NOTE: DAMN JACKSON!! ğŸ’–ğŸ˜\n",
      "#GOT7 \n",
      "#JACKSON \n",
      "#Jinyoung \n",
      "#ì­ìŠ¨ https://t.co/9ZlsEIXkITğŸ’–ğŸ˜\n",
      "----------------\n",
      "RT @papayaia: Jk really did THAT. Jimin playing along is so ğŸ˜ #BTSInHamilton https://t.co/LjoywCu0yGğŸ˜\n",
      "----------------\n",
      "RT @DAILYPUPPlES: Friendship ğŸ˜ https://t.co/E6TpHZs9GMğŸ˜\n",
      "----------------\n",
      "Grab all my faves and save our lives ğŸ˜ https://t.co/KGGUw4Qq6pğŸ˜\n",
      "----------------\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "id = 0\n",
    "for tweet in mynewlist:\n",
    "    emojis = extract_emojis(tweet)\n",
    "    a = set(emojis) & set(emoji_list)\n",
    "    if len(a)==1:\n",
    "        print(tweet + \"\".join(emojis))\n",
    "        print(\"----------------\")\n",
    "        count+=1\n",
    "    id += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @WORLDMUSICAWARD: Happy 26th Birthday to #EXO's very handsome and talented #CHEN! #HappyChenDay! @weareoneEXO\n",
      "â¤ï¸ğŸ‡°ğŸ‡·ğŸ¶ğŸ¤ğŸ‚ğŸ‰ğŸğŸˆğŸ˜ğŸŒŸğŸ’«ğŸ‡  \n",
      "https://tâ€¦\n",
      "RT @WORLDMUSICAWARD: Happy 26th Birthday to #EXO's very handsome and talented #CHEN! #HappyChenDay! @weareoneEXO\n",
      "â¤ï¸ğŸ‡°ğŸ‡·ğŸ¶ğŸ¤ğŸ‚ğŸ‰ğŸğŸˆğŸ˜ğŸŒŸğŸ’«ğŸ‡  \n",
      "https://tâ€¦\n",
      "RT @WORLDMUSICAWARD: Happy 26th Birthday to #EXO's very handsome and talented #CHEN! #HappyChenDay! @weareoneEXO\n",
      "â¤ï¸ğŸ‡°ğŸ‡·ğŸ¶ğŸ¤ğŸ‚ğŸ‰ğŸğŸˆğŸ˜ğŸŒŸğŸ’«ğŸ‡  \n",
      "https://tâ€¦\n"
     ]
    }
   ],
   "source": [
    "print(Tweets[11])\n",
    "print(Tweets[13])\n",
    "print(Tweets[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+'ğŸ˜®'+'.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    Tweets = pickle.load(fp)\n",
    "T = []\n",
    "T3 = \"\"\n",
    "for tweet in Tweets:\n",
    "#     emojis = extract_emojis(tweet)\n",
    "#     text = re.sub(\"|\".join(emoji_list), \"\", tweet) # delete emojis from tweet\n",
    "    text = delete_emojis(tweet)\n",
    "    text = sanitize_tweets(text)\n",
    "    T.append(text)\n",
    "    T3 = T3 + text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = \"The car is driven on the road.\"\n",
    "\n",
    "S2 = \"The truck is driven on the highway.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check generated vocab here\n",
    "X = vectorizer.fit_transform([S1,S2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'driven', 'highway', 'is', 'on', 'road', 'the', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "f_names = vectorizer.get_feature_names()\n",
    "print(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.6043795515372431\n",
      "  (0, 0)\t0.42471718586982765\n",
      "  (0, 3)\t0.30218977576862155\n",
      "  (0, 1)\t0.30218977576862155\n",
      "  (0, 4)\t0.30218977576862155\n",
      "  (0, 5)\t0.42471718586982765\n",
      "  (1, 6)\t0.6043795515372431\n",
      "  (1, 3)\t0.30218977576862155\n",
      "  (1, 1)\t0.30218977576862155\n",
      "  (1, 4)\t0.30218977576862155\n",
      "  (1, 7)\t0.42471718586982765\n",
      "  (1, 2)\t0.42471718586982765\n",
      "(2, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "(2, 25)\n",
      "0.37930349280874964\n",
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 15, 18, 20, 21, 22, 23, 24],\n",
      "      dtype=int32))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-399d1ad33f5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m487\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# (temp)\n",
    "# print(X.max())\n",
    "# IX = np.nonzero(X[1,:]>0.1)\n",
    "# type(IX)\n",
    "# print(IX)\n",
    "# print(f_names[487])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make 75-dim target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ğŸ˜€': 0, 'ğŸ˜': 1, 'ğŸ˜‚': 2, 'ğŸ¤£': 3, 'ğŸ˜ƒ': 4, 'ğŸ˜„': 5, 'ğŸ˜…': 6, 'ğŸ˜†': 7, 'ğŸ˜‰': 8, 'ğŸ˜Š': 9, 'ğŸ˜‹': 10, 'ğŸ˜': 11, 'ğŸ˜': 12, 'ğŸ˜˜': 13, 'ğŸ˜—': 14, 'ğŸ˜™': 15, 'ğŸ˜š': 16, 'â˜ºï¸': 17, 'ğŸ™‚': 18, 'ğŸ¤—': 19, 'ğŸ¤”': 20, 'ğŸ˜': 21, 'ğŸ˜‘': 22, 'ğŸ˜¶': 23, 'ğŸ™„': 24, 'ğŸ˜': 25, 'ğŸ˜£': 26, 'ğŸ˜¥': 27, 'ğŸ˜®': 28, 'ğŸ¤': 29, 'ğŸ˜¯': 30, 'ğŸ˜ª': 31, 'ğŸ˜«': 32, 'ğŸ˜´': 33, 'ğŸ˜Œ': 34, 'ğŸ˜›': 35, 'ğŸ˜œ': 36, 'ğŸ˜': 37, 'ğŸ¤¤': 38, 'ğŸ˜’': 39, 'ğŸ˜“': 40, 'ğŸ˜”': 41, 'ğŸ˜•': 42, 'ğŸ™ƒ': 43, 'ğŸ¤‘': 44, 'ğŸ˜²': 45, 'â˜¹ï¸': 46, 'ğŸ™': 47, 'ğŸ˜–': 48, 'ğŸ˜': 49, 'ğŸ˜Ÿ': 50, 'ğŸ˜¤': 51, 'ğŸ˜¢': 52, 'ğŸ˜­': 53, 'ğŸ˜¦': 54, 'ğŸ˜§': 55, 'ğŸ˜¨': 56, 'ğŸ˜©': 57, 'ğŸ˜¬': 58, 'ğŸ˜°': 59, 'ğŸ˜±': 60, 'ğŸ˜³': 61, 'ğŸ˜µ': 62, 'ğŸ˜¡': 63, 'ğŸ˜ ': 64, 'ğŸ˜·': 65, 'ğŸ¤’': 66, 'ğŸ¤•': 67, 'ğŸ¤¢': 68, 'ğŸ¤§': 69, 'ğŸ˜‡': 70, 'ğŸ¤ ': 71, 'ğŸ¤¡': 72, 'ğŸ¤¥': 73, 'ğŸ¤“': 74}\n"
     ]
    }
   ],
   "source": [
    "N = []\n",
    "for ii in range(75):\n",
    "    N.append(ii)\n",
    "Edict = dict(zip(emoji_list, N))\n",
    "print(Edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test code: make 75-D target\n",
    "# target_arr = np.zeros((10, 75))\n",
    "# for ii in range(10):\n",
    "#     e_list = D.raw_target[ii+10]\n",
    "#     for x in e_list:\n",
    "#         if x in emoji_list:\n",
    "#             ix = Edict[x]\n",
    "#             target_arr[ii,ix] = 1\n",
    "# #D.target = target_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜€\n",
      "ğŸ˜\n",
      "ğŸ˜‚\n",
      "ğŸ¤£\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜…\n",
      "ğŸ˜†\n",
      "ğŸ˜‰\n",
      "ğŸ˜Š\n",
      "ğŸ˜‹\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜˜\n",
      "ğŸ˜—\n",
      "ğŸ˜™\n",
      "ğŸ˜š\n",
      "â˜ºï¸\n",
      "ğŸ™‚\n",
      "ğŸ¤—\n",
      "ğŸ¤”\n",
      "ğŸ˜\n",
      "ğŸ˜‘\n",
      "ğŸ˜¶\n",
      "ğŸ™„\n",
      "ğŸ˜\n",
      "ğŸ˜£\n",
      "ğŸ˜¥\n",
      "ğŸ˜®\n",
      "ğŸ¤\n",
      "ğŸ˜¯\n",
      "ğŸ˜ª\n",
      "ğŸ˜«\n",
      "ğŸ˜´\n",
      "ğŸ˜Œ\n",
      "ğŸ˜›\n",
      "ğŸ˜œ\n",
      "ğŸ˜\n",
      "ğŸ¤¤\n",
      "ğŸ˜’\n",
      "ğŸ˜“\n",
      "ğŸ˜”\n",
      "ğŸ˜•\n",
      "ğŸ™ƒ\n",
      "ğŸ¤‘\n",
      "ğŸ˜²\n",
      "â˜¹ï¸\n",
      "ğŸ™\n",
      "ğŸ˜–\n",
      "ğŸ˜\n",
      "ğŸ˜Ÿ\n",
      "ğŸ˜¤\n",
      "ğŸ˜¢\n",
      "ğŸ˜­\n",
      "ğŸ˜¦\n",
      "ğŸ˜§\n",
      "ğŸ˜¨\n",
      "ğŸ˜©\n",
      "ğŸ˜¬\n",
      "ğŸ˜°\n",
      "ğŸ˜±\n",
      "ğŸ˜³\n",
      "ğŸ˜µ\n",
      "ğŸ˜¡\n",
      "ğŸ˜ \n",
      "ğŸ˜·\n",
      "ğŸ¤’\n",
      "ğŸ¤•\n",
      "ğŸ¤¢\n",
      "ğŸ¤§\n",
      "ğŸ˜‡\n",
      "ğŸ¤ \n",
      "ğŸ¤¡\n",
      "ğŸ¤¥\n",
      "ğŸ¤“\n"
     ]
    }
   ],
   "source": [
    "# 75-D preprocessing\n",
    "# IN\n",
    "target_names = ['ğŸ˜','ğŸ˜¡'] # emoji_list \n",
    "\n",
    "# OUT\n",
    "rawdata = []\n",
    "data = []\n",
    "raw_target = []\n",
    "Len = []\n",
    "filenames = []\n",
    "target_arr = np.zeros((5000*75, 75))\n",
    "search_emoji = []\n",
    "\n",
    "# main loop\n",
    "ii = 0\n",
    "for keyword in target_names:\n",
    "    print(keyword)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets = pickle.load(fp)\n",
    "        Len.append(len(Tweets))\n",
    "        \n",
    "        Text = []\n",
    "        RawText = []\n",
    "        E = []\n",
    "        for tweet in Tweets:\n",
    "            # save raw tweet\n",
    "            RawText.append(tweet)\n",
    "            # take out all emojis from input tweets\n",
    "            emojis = extract_emojis(tweet)\n",
    "            E.append(emojis)\n",
    "            \n",
    "            # save emojis into target array \n",
    "            for x in emojis:\n",
    "                if x in emoji_list:\n",
    "                    ix = Edict[x]\n",
    "                    target_arr[ii,ix] = 1\n",
    "            \n",
    "            # save sanitized text\n",
    "            text = sanitize_tweets(tweet)\n",
    "            text = delete_emojis(text)\n",
    "            Text.append(text)\n",
    "            ii = ii+1\n",
    "            \n",
    "        data = data + Text\n",
    "        rawdata = rawdata + RawText\n",
    "        raw_target = raw_target + E\n",
    "        filenames = filenames + [fullfile]\n",
    "        search_emoji = search_emoji + [keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional now: get 1-D target\n",
    "numEmojis = len(target_names)\n",
    "arr = []\n",
    "for i in range(numEmojis):\n",
    "    arr.extend([i] * Len[i]) \n",
    "target_1d = np.array(arr, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1437, 2168, 2621, 1432, 1921, 2062, 1689, 1481, 1948, 2214, 2154, 1968, 2228, 2137, 1917, 1931, 1825, 2250, 2353, 2114, 1584, 2378, 2588, 1260, 2804, 2244, 2210, 2240, 1816, 1117, 1960, 2979, 2101, 2480, 2667, 2490, 1733, 2078, 2434, 2711, 2361, 3098, 2532, 2841, 2141, 1525, 2991, 2445, 2365, 2871, 1809, 2308, 1677, 1696, 1583, 1944, 2084, 2017, 2174, 1105, 962, 1550, 1744, 1446, 2042, 2017, 2470, 798, 2833, 2261, 1869, 2109, 1507, 2266, 2245]\n"
     ]
    }
   ],
   "source": [
    "print(Len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this struct is saving relatively raw data, for later use ~ visualizations\n",
    "class tweet_data:\n",
    "    pass\n",
    "\n",
    "D = tweet_data()\n",
    "D.raw_data = rawdata\n",
    "D.data = data\n",
    "D.raw_target = raw_target\n",
    "D.filenames = filenames\n",
    "D.numTweets = Len\n",
    "D.target_1d = target_1d\n",
    "D.target_arr = target_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155410"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_data_75xsingles.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(D, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-94a0bbfd8c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target_arr, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be directly loaded to train model\n",
    "class train_data:\n",
    "    pass\n",
    "\n",
    "T = train_data()\n",
    "T.data = data\n",
    "# X = vectorizer.fit_transform(data)\n",
    "# T.X = X\n",
    "# T.feature_names = vectorizer.get_feature_names()\n",
    "# T.X_shape = X.shape\n",
    "# T.Y = target_arr\n",
    "T.y = target_1d\n",
    "T.target_names = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'train_data_75xsingles.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(T, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (old) small model with only 2 opposing emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1-D preprecessing\n",
    "# # IN\n",
    "# target_names = ['ğŸ˜','ğŸ˜¡'] # emoji_list # \n",
    "\n",
    "# # OUT\n",
    "# rawdata = []\n",
    "# data = []\n",
    "# raw_target = []\n",
    "# Len = []\n",
    "# filenames = []\n",
    "\n",
    "# # main loop\n",
    "# for keyword in target_names:\n",
    "#     fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "#     with open(fullfile, 'rb') as fp:\n",
    "#         itemlist = pickle.load(fp)\n",
    "#         Len.append(len(itemlist))\n",
    "        \n",
    "#         T = []\n",
    "#         RawT = []\n",
    "#         E = []\n",
    "#         for tweet in itemlist:\n",
    "#             # save raw tweet\n",
    "#             RawT.append(tweet)\n",
    "#             # take out all emojis from input tweets\n",
    "#             emojis = extract_emojis(tweet)\n",
    "#             E.append(emojis)\n",
    "#             # save sanitized text\n",
    "#             text = re.sub(\"|\".join(emoji_list), \"\", tweet)\n",
    "#             T.append(text)\n",
    "#         data = data + T\n",
    "#         rawdata = rawdata + RawT\n",
    "#         raw_target = raw_target + E\n",
    "#         filenames = filenames + [fullfile]\n",
    "        \n",
    "\n",
    "# class tweet_data:\n",
    "#     pass\n",
    "\n",
    "# D = tweet_data()\n",
    "# D.raw_data = rawdata\n",
    "# D.data = data\n",
    "# D.raw_target = raw_target\n",
    "# D.filesnames = filenames\n",
    "# D.numTweets = Len\n",
    "\n",
    "# # fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweets_2x5k.p')\n",
    "# # #fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweets_75x5k.p')\n",
    "# # with open(fullfile, 'wb') as fp:\n",
    "# #     pickle.dump(D, fp)\n",
    "\n",
    "\n",
    "# class tweet_train:\n",
    "#     pass\n",
    "# T = tweet_train()\n",
    "# T.target_names = target_names\n",
    "# T.data = D.data\n",
    "# T.filenames = filenames\n",
    "# T.target = target\n",
    "\n",
    "# fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_train_2x5k.p')\n",
    "# with open(fullfile, 'wb') as fp:\n",
    "#     pickle.dump(T, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 10, 2, 1, 21, 47, 661600)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"ğŸ˜ â¤ï¸ğŸ¶ğŸ¤ğŸ‚ğŸ‰ğŸğŸˆğŸ˜ğŸŒŸğŸ’« shsha;lsdij \"\n",
    "emojis = extract_emojis(tweet)\n",
    "print(emojis)\n",
    "a = set(emojis) & set(emoji_list)\n",
    "a2 = list(a)\n",
    "print(a)\n",
    "print(len(a2)==1 and a2[0]=='ğŸ˜')\n",
    "print(a2[0]=='ğŸ˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = Tweets[:100]\n",
    "myset = set(mylist)\n",
    "mynewlist = list(myset)\n",
    "print(len(mynewlist))\n",
    "for tweet in mynewlist:\n",
    "    emojis = extract_emojis(tweet)\n",
    "    print(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "id = 0\n",
    "for tweet in mynewlist:\n",
    "    emojis = extract_emojis(tweet)\n",
    "    a = set(emojis) & set(emoji_list)\n",
    "    if len(a)==1:\n",
    "        print(tweet + \"\".join(emojis))\n",
    "        print(\"----------------\")\n",
    "        count+=1\n",
    "    id += 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
