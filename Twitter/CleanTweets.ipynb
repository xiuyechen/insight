{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            \n",
    "            text = re.sub(\"|\".join(word), \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😀', '😁', '😂', '🤣', '😃', '😄', '😅', '😆', '😉', '😊', '😋', '😎', '😍', '😘', '😗', '😙', '😚', '☺️', '🙂', '🤗', '🤔', '😐', '😑', '😶', '🙄', '😏', '😣', '😥', '😮', '🤐', '😯', '😪', '😫', '😴', '😌', '😛', '😜', '😝', '🤤', '😒', '😓', '😔', '😕', '🙃', '🤑', '😲', '☹️', '🙁', '😖', '😞', '😟', '😤', '😢', '😭', '😦', '😧', '😨', '😩', '😬', '😰', '😱', '😳', '😵', '😡', '😠', '😷', '🤒', '🤕', '🤢', '🤧', '😇', '🤠', '🤡', '🤥', '🤓']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# load emoji list\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Emoji/\"+'mySmileys.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    emoji_list = pickle.load(fp)\n",
    "print(emoji_list)\n",
    "print(len(emoji_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing: word-to-vec with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     and other words! '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize_tweets(tweet):\n",
    "    # delete twitter-specific tags\n",
    "    b1 = re.findall(r'@\\S*', tweet) # Twitter user names\n",
    "    b2 = re.findall(r'RT\\S*', tweet) # \"RT\"\n",
    "    b3 = re.findall(r'http\\S*', tweet) # links\n",
    "\n",
    "    c = tweet #list(tweet.split())\n",
    "    for b in b1:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b2:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b3:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    return c\n",
    "    \n",
    "tweet = \"RT @aa RT @bbb https:__ and other words! httpsaa\" #RawT[-1]\n",
    "sanitize_tweets(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word',# min_df=3, ngram_range=(1, 3)\n",
    "                             max_features = 2000, #stop_words='english',\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\")\n",
    "\n",
    "# https://stackoverflow.com/questions/39254134/how-to-preserve-punctuation-marks-in-scikit-learn-text-countvectorizer-or-tfidfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+'😱'+'.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    Tweets = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😍', '❤️', '🎶', '🎤', '🎂', '🎉', '🎁', '🎈', '😍', '🌟', '💫']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "tweet = \"😍 ❤️🎶🎤🎂🎉🎁🎈😍🌟💫 shsha;lsdij \"\n",
    "emojis = extract_emojis(tweet)\n",
    "print(emojis)\n",
    "a = set(emojis) & set(emoji_list)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @aMrazing: HOLYMOTHER OF ALL TWISTS OMFG NAGINI WAS A WOMAN LIKE WHAT THE HELL 😱😱😱😱😱\n",
      "KALIAN TONTON DONG TRAILER BARU FANTASTIC BEASTS TE…😱😱😱😱😱\n",
      "----------------\n",
      "@mizz_nayomie Your iPhone😱😱\n",
      "----------------\n",
      "RT @ChiSportUpdates: 😱CHICAGOOOOOO😱\n",
      "\n",
      "(Picture via Chicago Bulls) https://t.co/gpqmogYaYP😱😱\n",
      "----------------\n",
      "RT @dinahjane97: 😱😱😱😱😱😱😱😱😱😱 OH MAhhh GAHHH!!! LOVE LOVE LOVE YOU GUYS X glad y’all diggin it 🍾 https://t.co/vXIuOfdUsy😱😱😱😱😱😱😱😱😱😱🍾\n",
      "----------------\n",
      "RT @dodo: These people CANNOT believe the wild animals they’re running into 😱 https://t.co/vE0MiJaoG7😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @OnlineMagazin: 🆘‼😱🚖 #Spain: Ouch! Oh dear! On the Spanish island of Ibiza, an African physicist was hit by a racist Spanish taxi driver…🆘‼😱🚖\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @itsIvanOk: NASA just released this video of how the planets sound like from outer space 😱 https://t.co/r2388kFdND😱\n",
      "----------------\n",
      "RT @BangtanTrends: #BTSonFallon is now trending #2 WW with over 1M tweets 😱\n",
      "\n",
      "Can we get it trending to #1?! \n",
      "\n",
      "(@BTS_twt)😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @BBCEarth: You won't believe what this puffer fish creates 😱 💙\n",
      "\n",
      "#LifeStory https://t.co/npTEYcpA1v😱💙\n",
      "----------------\n",
      "RT @DcruzeV: @Deeply_Crazy yeah just came to know today morning... #Aarohi torturing Tara. 😱😱.. For the past few months I thought it was vi…😱😱\n",
      "----------------\n",
      "@xUnaCajax Achis, donde dice? 😱😱\n",
      "----------------\n",
      "RT @AhgaWang1: Waah my body hair is up 😱\n",
      "#GOT7 #Lullaby #PresentYOU #갓세븐 @GOT7Official \n",
      "Sorum ... goosebumps... https://t.co/CzO3vvyWwt😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @Sporf: 🏴󠁧󠁢󠁥󠁮󠁧󠁿 Frank Lampard’s last 5 days:\n",
      "\n",
      "📅 Friday\n",
      "👶 Announces Birth of his Daughter \n",
      "\n",
      "📅 Saturday\n",
      "✅ Beats Brentford 3-1\n",
      "\n",
      "📅 Tuesday\n",
      "😱…🏴󠁧󠁢󠁥󠁮󠁧󠁿📅👶📅✅📅😱\n",
      "----------------\n",
      "RT @lourmith: @SHELAVSTAN Ate She Oct. ba or Dec?😱\n",
      "\n",
      "#ALDUBADNMissesYou😱\n",
      "----------------\n",
      "RT @BangtanTrends: #BTSonFallon is now trending #2 WW with over 1M tweets 😱\n",
      "\n",
      "Can we get it trending to #1?! \n",
      "\n",
      "(@BTS_twt)😱\n",
      "----------------\n",
      "Defense Secretary Jim Mattis Says 'Jury Out' on Whether Women Can Succeed in Military Combat - TIME😱No, we don’t ha… https://t.co/nqWV8CBtPi😱\n",
      "----------------\n",
      "I liked a @YouTube video https://t.co/beWxVm9ZtM DROP MAP KE ANDR CHLA GYA WTF😱!! WTF II FUNNY MOMENT PUBGMOBILE # HIGHLIGHTS😱\n",
      "----------------\n",
      "RT @Seebbes: If this Blackpink x League of Legends rumor is real holy moly that would be HUGE😱\n",
      "\n",
      "It's obviously just a rumor but some say th…😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "@saramachhoor @bulldog5278 @_Matchstic @JunesIphone Wall plz😱😱\n",
      "----------------\n",
      "RT @BangtanTrends: #BTSonFallon is now trending #2 WW with over 1M tweets 😱\n",
      "\n",
      "Can we get it trending to #1?! \n",
      "\n",
      "(@BTS_twt)😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @Mens_Corner_: Something kept eating this farmers chickens. So he set a trap, and this is what he caught...😱😱 https://t.co/9LoekGXRm3😱😱\n",
      "----------------\n",
      "Survey!\n",
      "❤ if you love Wattpad.\n",
      "😱 if not.\n",
      "-WIL Editor❤😱\n",
      "----------------\n",
      "RT @EW: #FantasticBeasts actress Claudia Kim is....Nagini! 😱🐍 See more character photos here: https://t.co/vMQn4HcbwQ https://t.co/WmvRi6z7…😱🐍\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @FootballVines: Teachers vs Students match &amp; then one of the teachers does this... 😱 https://t.co/EoNGmoQghh😱\n",
      "----------------\n",
      "Dr. Max Goodwin have cancer! 😱😱\n",
      "#NewAmsterdam😱😱\n",
      "----------------\n",
      "Tfw you add a new friend on Facebook and you 🤔 start 🙋🏼‍♀️ seeing all 👉🏼 the 🤫 emoji 🖊 posts 🙀 and 👏🏼 oh 🙏🏼 god 💸 o… https://t.co/4gMgNChdTB🤔🙋🏼‍♀️👉🏼🤫🖊🙀👏🏼🙏🏼💸\n",
      "----------------\n",
      "RT @lahzerous: PLZ Tell me this is @jimmyfallon  doing \"Go Go\"!!!! 😱💜 @BTS_twt #BTSonFallon https://t.co/wIgqcDh1Xk😱💜\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @Mens_Corner_: Something kept eating this farmers chickens. So he set a trap, and this is what he caught...😱😱 https://t.co/9LoekGXRm3😱😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @AmyRoseCapetta: VICTORIA SCHWAB HAS AN ARC OF OUR BOOK! 😱\n",
      "\n",
      "In celebration of VENGEFUL's book birthday and this special sighting of ONCE…😱\n",
      "----------------\n",
      "RT @seunghanyoon: 180916 Mingyu baby your biceps 😱😱 \n",
      "\n",
      "#IDEALCUTinJKT #IDEALCUTinJAKARTA #SEVENTEEN #MINGYU \n",
      "#세븐틴 #민규 https://t.co/zoWwSYuQFs😱😱\n",
      "----------------\n",
      "Thats wrong in so many levels... 😱 https://t.co/SmaouoyG2U😱\n",
      "----------------\n",
      "RT @Sporf: 🏴󠁧󠁢󠁥󠁮󠁧󠁿 Frank Lampard’s last 5 days:\n",
      "\n",
      "📅 Friday\n",
      "👶 Announces Birth of his Daughter \n",
      "\n",
      "📅 Saturday\n",
      "✅ Beats Brentford 3-1\n",
      "\n",
      "📅 Tuesday\n",
      "😱…🏴󠁧󠁢󠁥󠁮󠁧󠁿📅👶📅✅📅😱\n",
      "----------------\n",
      "O M G 😱 Please give me willis at dog money🙏🙏 https://t.co/dsQgToRwjX😱🙏🙏\n",
      "----------------\n",
      "RT @kelsey0315: GIANT??😱😱 #KrisWu @KrisWu https://t.co/J9CiJXUgDW😱😱\n",
      "----------------\n",
      "RT @AhgaWang1: this sunshine is OMG😱\n",
      "#GOT7 #Lullaby #PresentYOU #갓세븐 @GOT7Official https://t.co/wksEPoDC6I😱\n",
      "----------------\n",
      "Hour left until our Tuesday Moonrock deal is over 😱 https://t.co/LFDFnRl8dc😱\n",
      "----------------\n",
      "Eight am is the sound of souls dying. 😱😱😱😱😱😱\n",
      "----------------\n",
      "RT @BangtanTrends: #BTSonFallon is now trending #2 WW with over 1M tweets 😱\n",
      "\n",
      "Can we get it trending to #1?! \n",
      "\n",
      "(@BTS_twt)😱\n",
      "----------------\n",
      "@msdanifernandez The emoji movie is pretty scary 😱😱\n",
      "----------------\n",
      "RT @5inclate: 300 SECOND SKULL TROOPER ACCOUNT GIVEAWAY😱💚\n",
      "\n",
      "To enter:\n",
      "Retweet\n",
      "Follow @TheTeamCurse,@lflizas,@Posterify,@Remulse \n",
      "Turn on not…😱💚\n",
      "----------------\n",
      "RT @BangtanTrends: #BTSonFallon is now trending #2 WW with over 1M tweets 😱\n",
      "\n",
      "Can we get it trending to #1?! \n",
      "\n",
      "(@BTS_twt)😱\n",
      "----------------\n",
      "RT @dinahjane97: @DinahCharts 🙈🙈🙈😱😱😱yyaaasssssss Fiji !!!!! Hehe time to make a trip? Lol🙈🙈🙈😱😱😱\n",
      "----------------\n",
      "RT @Mens_Corner_: Something kept eating this farmers chickens. So he set a trap, and this is what he caught...😱😱 https://t.co/9LoekGXRm3😱😱\n",
      "----------------\n",
      "@hxrley__ Derby now, along with imps and Chelsea!😱😱\n",
      "----------------\n",
      "😱 #Soldes\n",
      "    \n",
      "🎮 Air Conflicts: Vietnam ▶ https://t.co/RqdrvZZfEV\r\n",
      "\n",
      "#BonPlan https://t.co/uPVrZ84T6u😱🎮▶\n",
      "----------------\n",
      "@BartonTheFox @SvenFennec Birthday punches?! 😱😱\n",
      "----------------\n",
      "Fuck 😱😱😱 https://t.co/teyWpr7jsD😱😱😱\n",
      "----------------\n",
      "RT @buitengebieden: 😱\n",
      "\n",
      "A racoon had lucky escape after jumping from an apartment block and living to tell the tail.\n",
      "\n",
      "#racoon @TomHall #500p…😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "@br00t4l1ty WHAT IS THIS BLASPHEMY 😱😱\n",
      "----------------\n",
      "RT @BTSxFlorida: ARMY we have less than 30 minutes 😱 keep voting for #IDOL by @BTS_twt ft. @NICKIMINAJ on the #BETHEBOSS poll on @B1039Radi…😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "Wait, I wasn't prepared 😱😱😱 https://t.co/FvJgRYs458😱😱😱\n",
      "----------------\n",
      "RT @FortniteFunny: Holy!! 😱😱\n",
      "\n",
      "Credit @ObeyGorb https://t.co/yzWUs0YZIO😱😱\n",
      "----------------\n",
      "RT @BangtanTrends: #BTSonFallon is now trending #2 WW with over 1M tweets 😱\n",
      "\n",
      "Can we get it trending to #1?! \n",
      "\n",
      "(@BTS_twt)😱\n",
      "----------------\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "id = 0\n",
    "for tweet in Tweets[:100]:\n",
    "    emojis = extract_emojis(tweet)\n",
    "    a = set(emojis) & set(emoji_list)\n",
    "    if len(a)==1:\n",
    "        print(tweet + \"\".join(emojis))\n",
    "        print(\"----------------\")\n",
    "        count+=1\n",
    "    id += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @WORLDMUSICAWARD: Happy 26th Birthday to #EXO's very handsome and talented #CHEN! #HappyChenDay! @weareoneEXO\n",
      "❤️🇰🇷🎶🎤🎂🎉🎁🎈😍🌟💫🎇  \n",
      "https://t…\n",
      "RT @WORLDMUSICAWARD: Happy 26th Birthday to #EXO's very handsome and talented #CHEN! #HappyChenDay! @weareoneEXO\n",
      "❤️🇰🇷🎶🎤🎂🎉🎁🎈😍🌟💫🎇  \n",
      "https://t…\n",
      "RT @WORLDMUSICAWARD: Happy 26th Birthday to #EXO's very handsome and talented #CHEN! #HappyChenDay! @weareoneEXO\n",
      "❤️🇰🇷🎶🎤🎂🎉🎁🎈😍🌟💫🎇  \n",
      "https://t…\n"
     ]
    }
   ],
   "source": [
    "print(Tweets[11])\n",
    "print(Tweets[13])\n",
    "print(Tweets[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+'😮'+'.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    Tweets = pickle.load(fp)\n",
    "T = []\n",
    "T3 = \"\"\n",
    "for tweet in Tweets:\n",
    "#     emojis = extract_emojis(tweet)\n",
    "#     text = re.sub(\"|\".join(emoji_list), \"\", tweet) # delete emojis from tweet\n",
    "    text = delete_emojis(tweet)\n",
    "    text = sanitize_tweets(text)\n",
    "    T.append(text)\n",
    "    T3 = T3 + text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = \"The car is driven on the road.\"\n",
    "\n",
    "S2 = \"The truck is driven on the highway.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check generated vocab here\n",
    "X = vectorizer.fit_transform([S1,S2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'driven', 'highway', 'is', 'on', 'road', 'the', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "f_names = vectorizer.get_feature_names()\n",
    "print(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.6043795515372431\n",
      "  (0, 0)\t0.42471718586982765\n",
      "  (0, 3)\t0.30218977576862155\n",
      "  (0, 1)\t0.30218977576862155\n",
      "  (0, 4)\t0.30218977576862155\n",
      "  (0, 5)\t0.42471718586982765\n",
      "  (1, 6)\t0.6043795515372431\n",
      "  (1, 3)\t0.30218977576862155\n",
      "  (1, 1)\t0.30218977576862155\n",
      "  (1, 4)\t0.30218977576862155\n",
      "  (1, 7)\t0.42471718586982765\n",
      "  (1, 2)\t0.42471718586982765\n",
      "(2, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "(2, 25)\n",
      "0.37930349280874964\n",
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 15, 18, 20, 21, 22, 23, 24],\n",
      "      dtype=int32))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-399d1ad33f5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m487\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# (temp)\n",
    "# print(X.max())\n",
    "# IX = np.nonzero(X[1,:]>0.1)\n",
    "# type(IX)\n",
    "# print(IX)\n",
    "# print(f_names[487])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make 75-dim target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'😀': 0, '😁': 1, '😂': 2, '🤣': 3, '😃': 4, '😄': 5, '😅': 6, '😆': 7, '😉': 8, '😊': 9, '😋': 10, '😎': 11, '😍': 12, '😘': 13, '😗': 14, '😙': 15, '😚': 16, '☺️': 17, '🙂': 18, '🤗': 19, '🤔': 20, '😐': 21, '😑': 22, '😶': 23, '🙄': 24, '😏': 25, '😣': 26, '😥': 27, '😮': 28, '🤐': 29, '😯': 30, '😪': 31, '😫': 32, '😴': 33, '😌': 34, '😛': 35, '😜': 36, '😝': 37, '🤤': 38, '😒': 39, '😓': 40, '😔': 41, '😕': 42, '🙃': 43, '🤑': 44, '😲': 45, '☹️': 46, '🙁': 47, '😖': 48, '😞': 49, '😟': 50, '😤': 51, '😢': 52, '😭': 53, '😦': 54, '😧': 55, '😨': 56, '😩': 57, '😬': 58, '😰': 59, '😱': 60, '😳': 61, '😵': 62, '😡': 63, '😠': 64, '😷': 65, '🤒': 66, '🤕': 67, '🤢': 68, '🤧': 69, '😇': 70, '🤠': 71, '🤡': 72, '🤥': 73, '🤓': 74}\n"
     ]
    }
   ],
   "source": [
    "N = []\n",
    "for ii in range(75):\n",
    "    N.append(ii)\n",
    "Edict = dict(zip(emoji_list, N))\n",
    "print(Edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test code: make 75-D target\n",
    "# target_arr = np.zeros((10, 75))\n",
    "# for ii in range(10):\n",
    "#     e_list = D.raw_target[ii+10]\n",
    "#     for x in e_list:\n",
    "#         if x in emoji_list:\n",
    "#             ix = Edict[x]\n",
    "#             target_arr[ii,ix] = 1\n",
    "# #D.target = target_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😀\n",
      "😁\n",
      "😂\n",
      "🤣\n",
      "😃\n",
      "😄\n",
      "😅\n",
      "😆\n",
      "😉\n",
      "😊\n",
      "😋\n",
      "😎\n",
      "😍\n",
      "😘\n",
      "😗\n",
      "😙\n",
      "😚\n",
      "☺️\n",
      "🙂\n",
      "🤗\n",
      "🤔\n",
      "😐\n",
      "😑\n",
      "😶\n",
      "🙄\n",
      "😏\n",
      "😣\n",
      "😥\n",
      "😮\n",
      "🤐\n",
      "😯\n",
      "😪\n",
      "😫\n",
      "😴\n",
      "😌\n",
      "😛\n",
      "😜\n",
      "😝\n",
      "🤤\n",
      "😒\n",
      "😓\n",
      "😔\n",
      "😕\n",
      "🙃\n",
      "🤑\n",
      "😲\n",
      "☹️\n",
      "🙁\n",
      "😖\n",
      "😞\n",
      "😟\n",
      "😤\n",
      "😢\n",
      "😭\n",
      "😦\n",
      "😧\n",
      "😨\n",
      "😩\n",
      "😬\n",
      "😰\n",
      "😱\n",
      "😳\n",
      "😵\n",
      "😡\n",
      "😠\n",
      "😷\n",
      "🤒\n",
      "🤕\n",
      "🤢\n",
      "🤧\n",
      "😇\n",
      "🤠\n",
      "🤡\n",
      "🤥\n",
      "🤓\n"
     ]
    }
   ],
   "source": [
    "# 75-D preprecessing\n",
    "# IN\n",
    "target_names = ['😍','😡'] # emoji_list \n",
    "\n",
    "# OUT\n",
    "rawdata = []\n",
    "data = []\n",
    "raw_target = []\n",
    "Len = []\n",
    "filenames = []\n",
    "target_arr = np.zeros((5000*75, 75))\n",
    "search_emoji = []\n",
    "\n",
    "# main loop\n",
    "ii = 0\n",
    "for keyword in target_names:\n",
    "    print(keyword)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets = pickle.load(fp)\n",
    "        Len.append(len(Tweets))\n",
    "        \n",
    "        Text = []\n",
    "        RawText = []\n",
    "        E = []\n",
    "        for tweet in Tweets:\n",
    "            # save raw tweet\n",
    "            RawText.append(tweet)\n",
    "            # take out all emojis from input tweets\n",
    "            emojis = extract_emojis(tweet)\n",
    "            E.append(emojis)\n",
    "            \n",
    "            # save emojis into target array \n",
    "            for x in emojis:\n",
    "                if x in emoji_list:\n",
    "                    ix = Edict[x]\n",
    "                    target_arr[ii,ix] = 1\n",
    "            \n",
    "            # save sanitized text\n",
    "            text = sanitize_tweets(tweet)\n",
    "            text = delete_emojis(text)\n",
    "            Text.append(text)\n",
    "            ii = ii+1\n",
    "            \n",
    "        data = data + Text\n",
    "        rawdata = rawdata + RawText\n",
    "        raw_target = raw_target + E\n",
    "        filenames = filenames + [fullfile]\n",
    "        search_emoji = search_emoji + [keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional now: get 1-D target\n",
    "numEmojis = len(target_names)\n",
    "arr = []\n",
    "for i in range(numEmojis):\n",
    "    arr.extend([i] * Len[i]) \n",
    "target_1d = np.array(arr, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this struct is saving relatively raw data, for later use ~ visualizations\n",
    "class tweet_data:\n",
    "    pass\n",
    "\n",
    "D = tweet_data()\n",
    "D.raw_data = rawdata\n",
    "D.data = data\n",
    "D.raw_target = raw_target\n",
    "D.filenames = filenames\n",
    "D.numTweets = Len\n",
    "D.target_1d = target_1d\n",
    "D.target_arr = target_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_data_75x5k.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(D, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-94a0bbfd8c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target_arr, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be directly loaded to train model\n",
    "class train_data:\n",
    "    pass\n",
    "\n",
    "T = train_data()\n",
    "T.data = data\n",
    "# X = vectorizer.fit_transform(data)\n",
    "# T.X = X\n",
    "T.feature_names = vectorizer.get_feature_names()\n",
    "# T.X_shape = X.shape\n",
    "T.Y = target_arr\n",
    "T.y_1d = target_1d\n",
    "T.target_names = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'train_data_75x5k.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(T, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (old) small model with only 2 opposing emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1-D preprecessing\n",
    "# # IN\n",
    "# target_names = ['😍','😡'] # emoji_list # \n",
    "\n",
    "# # OUT\n",
    "# rawdata = []\n",
    "# data = []\n",
    "# raw_target = []\n",
    "# Len = []\n",
    "# filenames = []\n",
    "\n",
    "# # main loop\n",
    "# for keyword in target_names:\n",
    "#     fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "#     with open(fullfile, 'rb') as fp:\n",
    "#         itemlist = pickle.load(fp)\n",
    "#         Len.append(len(itemlist))\n",
    "        \n",
    "#         T = []\n",
    "#         RawT = []\n",
    "#         E = []\n",
    "#         for tweet in itemlist:\n",
    "#             # save raw tweet\n",
    "#             RawT.append(tweet)\n",
    "#             # take out all emojis from input tweets\n",
    "#             emojis = extract_emojis(tweet)\n",
    "#             E.append(emojis)\n",
    "#             # save sanitized text\n",
    "#             text = re.sub(\"|\".join(emoji_list), \"\", tweet)\n",
    "#             T.append(text)\n",
    "#         data = data + T\n",
    "#         rawdata = rawdata + RawT\n",
    "#         raw_target = raw_target + E\n",
    "#         filenames = filenames + [fullfile]\n",
    "        \n",
    "\n",
    "# class tweet_data:\n",
    "#     pass\n",
    "\n",
    "# D = tweet_data()\n",
    "# D.raw_data = rawdata\n",
    "# D.data = data\n",
    "# D.raw_target = raw_target\n",
    "# D.filesnames = filenames\n",
    "# D.numTweets = Len\n",
    "\n",
    "# # fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweets_2x5k.p')\n",
    "# # #fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweets_75x5k.p')\n",
    "# # with open(fullfile, 'wb') as fp:\n",
    "# #     pickle.dump(D, fp)\n",
    "\n",
    "\n",
    "# class tweet_train:\n",
    "#     pass\n",
    "# T = tweet_train()\n",
    "# T.target_names = target_names\n",
    "# T.data = D.data\n",
    "# T.filenames = filenames\n",
    "# T.target = target\n",
    "\n",
    "# fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_train_2x5k.p')\n",
    "# with open(fullfile, 'wb') as fp:\n",
    "#     pickle.dump(T, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8c2158971599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
