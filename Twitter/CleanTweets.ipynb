{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            \n",
    "            text = re.sub(\"|\".join(word), \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😀', '😁', '😂', '🤣', '😃', '😄', '😅', '😆', '😉', '😊', '😋', '😎', '😍', '😘', '😗', '😙', '😚', '☺️', '🙂', '🤗', '🤔', '😐', '😑', '😶', '🙄', '😏', '😣', '😥', '😮', '🤐', '😯', '😪', '😫', '😴', '😌', '😛', '😜', '😝', '🤤', '😒', '😓', '😔', '😕', '🙃', '🤑', '😲', '☹️', '🙁', '😖', '😞', '😟', '😤', '😢', '😭', '😦', '😧', '😨', '😩', '😬', '😰', '😱', '😳', '😵', '😡', '😠', '😷', '🤒', '🤕', '🤢', '🤧', '😇', '🤠', '🤡', '🤥', '🤓']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# load emoji list\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Emoji/\"+'mySmileys.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    emoji_list = pickle.load(fp)\n",
    "print(emoji_list)\n",
    "print(len(emoji_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sanitize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     and other words! '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize_tweets(tweet):\n",
    "    # delete twitter-specific tags\n",
    "    b1 = re.findall(r'@\\S*', tweet) # Twitter user names\n",
    "    b2 = re.findall(r'RT\\S*', tweet) # \"RT\"\n",
    "    b3 = re.findall(r'http\\S*', tweet) # links\n",
    "\n",
    "    c = tweet #list(tweet.split())\n",
    "    for b in b1:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b2:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b3:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    return c\n",
    "    \n",
    "tweet = \"RT @aa RT @bbb https:__ and other words! httpsaa\" #RawT[-1]\n",
    "sanitize_tweets(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😀\n",
      "😁\n",
      "😂\n",
      "🤣\n",
      "😃\n",
      "😄\n",
      "😅\n",
      "😆\n",
      "😉\n",
      "😊\n",
      "😋\n",
      "😎\n",
      "😍\n",
      "😘\n",
      "😗\n",
      "😙\n",
      "😚\n",
      "☺️\n",
      "🙂\n",
      "🤗\n",
      "🤔\n",
      "😐\n",
      "😑\n",
      "😶\n",
      "🙄\n",
      "😏\n",
      "😣\n",
      "😥\n",
      "😮\n",
      "🤐\n",
      "😯\n",
      "😪\n",
      "😫\n",
      "😴\n",
      "😌\n",
      "😛\n",
      "😜\n",
      "😝\n",
      "🤤\n",
      "😒\n",
      "😓\n",
      "😔\n",
      "😕\n",
      "🙃\n",
      "🤑\n",
      "😲\n",
      "☹️\n",
      "🙁\n",
      "😖\n",
      "😞\n",
      "😟\n",
      "😤\n",
      "😢\n",
      "😭\n",
      "😦\n",
      "😧\n",
      "😨\n",
      "😩\n",
      "😬\n",
      "😰\n",
      "😱\n",
      "😳\n",
      "😵\n",
      "😡\n",
      "😠\n",
      "😷\n",
      "🤒\n",
      "🤕\n",
      "🤢\n",
      "🤧\n",
      "😇\n",
      "🤠\n",
      "🤡\n",
      "🤥\n",
      "🤓\n"
     ]
    }
   ],
   "source": [
    "# preprocessing with removing repeat tweets\n",
    "# IN\n",
    "target_names = emoji_list # ['😍','😡'] # \n",
    "\n",
    "# OUT\n",
    "rawdata = []\n",
    "data = []\n",
    "raw_target = []\n",
    "Len = []\n",
    "filenames = []\n",
    "target_arr = np.zeros((5000*75*2, 75)) # init bigger than actual\n",
    "search_emoji = []\n",
    "\n",
    "# main loop\n",
    "ii = 0\n",
    "for keyword in target_names:\n",
    "    print(keyword)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets1 = pickle.load(fp)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets2 = pickle.load(fp)\n",
    "    Tweets = Tweets1+Tweets2\n",
    "\n",
    "    # remove repeat tweets\n",
    "    myset = set(Tweets)\n",
    "    Tweets_unique = list(myset)\n",
    "\n",
    "    Text = []\n",
    "    RawText = []\n",
    "    E = []\n",
    "    for tweet in Tweets_unique:\n",
    "        # remove tweets with more than one smiley\n",
    "#             emojis = extract_emojis(tweet)\n",
    "#             a = set(emojis) & set(emoji_list)\n",
    "#             a2 = list(a)\n",
    "#             if len(a2)==1 and a2[0]==keyword: # else: deal with later if doing multi-D target\n",
    "\n",
    "        # save raw tweet\n",
    "        RawText.append(tweet)\n",
    "        # save all emojis from input tweets\n",
    "        emojis = extract_emojis(tweet)\n",
    "        E.append(emojis)\n",
    "\n",
    "        # save emojis into target array \n",
    "        for x in emojis:\n",
    "            if x in emoji_list:\n",
    "                ix = Edict[x]\n",
    "                target_arr[ii,ix] = 1\n",
    "\n",
    "        # save sanitized text\n",
    "        text = sanitize_tweets(tweet)\n",
    "        text = delete_emojis(text)\n",
    "        Text.append(text)\n",
    "        ii = ii+1\n",
    "\n",
    "    data = data + Text\n",
    "    rawdata = rawdata + RawText\n",
    "    raw_target = raw_target + E\n",
    "    filenames = filenames + [fullfile]\n",
    "    search_emoji = search_emoji + [keyword]\n",
    "    Len.append(len(Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make 75-dim target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'😀': 0, '😁': 1, '😂': 2, '🤣': 3, '😃': 4, '😄': 5, '😅': 6, '😆': 7, '😉': 8, '😊': 9, '😋': 10, '😎': 11, '😍': 12, '😘': 13, '😗': 14, '😙': 15, '😚': 16, '☺️': 17, '🙂': 18, '🤗': 19, '🤔': 20, '😐': 21, '😑': 22, '😶': 23, '🙄': 24, '😏': 25, '😣': 26, '😥': 27, '😮': 28, '🤐': 29, '😯': 30, '😪': 31, '😫': 32, '😴': 33, '😌': 34, '😛': 35, '😜': 36, '😝': 37, '🤤': 38, '😒': 39, '😓': 40, '😔': 41, '😕': 42, '🙃': 43, '🤑': 44, '😲': 45, '☹️': 46, '🙁': 47, '😖': 48, '😞': 49, '😟': 50, '😤': 51, '😢': 52, '😭': 53, '😦': 54, '😧': 55, '😨': 56, '😩': 57, '😬': 58, '😰': 59, '😱': 60, '😳': 61, '😵': 62, '😡': 63, '😠': 64, '😷': 65, '🤒': 66, '🤕': 67, '🤢': 68, '🤧': 69, '😇': 70, '🤠': 71, '🤡': 72, '🤥': 73, '🤓': 74}\n"
     ]
    }
   ],
   "source": [
    "N = []\n",
    "for ii in range(75):\n",
    "    N.append(ii)\n",
    "Edict = dict(zip(emoji_list, N))\n",
    "print(Edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😀\n",
      "😁\n",
      "😂\n",
      "🤣\n",
      "😃\n",
      "😄\n",
      "😅\n",
      "😆\n",
      "😉\n",
      "😊\n",
      "😋\n",
      "😎\n",
      "😍\n",
      "😘\n",
      "😗\n",
      "😙\n",
      "😚\n",
      "☺️\n",
      "🙂\n",
      "🤗\n",
      "🤔\n",
      "😐\n",
      "😑\n",
      "😶\n",
      "🙄\n",
      "😏\n",
      "😣\n",
      "😥\n",
      "😮\n",
      "🤐\n",
      "😯\n",
      "😪\n",
      "😫\n",
      "😴\n",
      "😌\n",
      "😛\n",
      "😜\n",
      "😝\n",
      "🤤\n",
      "😒\n",
      "😓\n",
      "😔\n",
      "😕\n",
      "🙃\n",
      "🤑\n",
      "😲\n",
      "☹️\n",
      "🙁\n",
      "😖\n",
      "😞\n",
      "😟\n",
      "😤\n",
      "😢\n",
      "😭\n",
      "😦\n",
      "😧\n",
      "😨\n",
      "😩\n",
      "😬\n",
      "😰\n",
      "😱\n",
      "😳\n",
      "😵\n",
      "😡\n",
      "😠\n",
      "😷\n",
      "🤒\n",
      "🤕\n",
      "🤢\n",
      "🤧\n",
      "😇\n",
      "🤠\n",
      "🤡\n",
      "🤥\n",
      "🤓\n"
     ]
    }
   ],
   "source": [
    "# 75-D preprocessing\n",
    "# IN\n",
    "target_names = emoji_list \n",
    "\n",
    "# OUT\n",
    "rawdata = []\n",
    "data = []\n",
    "raw_target = []\n",
    "Len = []\n",
    "filenames = []\n",
    "target_arr = np.zeros((5000*75*2, 75))\n",
    "# search_emoji = []\n",
    "\n",
    "# main loop\n",
    "ii = 0\n",
    "for keyword in target_names:\n",
    "    print(keyword)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets1 = pickle.load(fp)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k_batch2/\"+'outfile'+keyword)\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets2 = pickle.load(fp)\n",
    "    Tweets = Tweets1 + Tweets2\n",
    "    \n",
    "    Len.append(len(Tweets))\n",
    "\n",
    "    Text = []\n",
    "    RawText = []\n",
    "    E = []\n",
    "    for tweet in Tweets:\n",
    "        # save raw tweet\n",
    "        RawText.append(tweet)\n",
    "        # take out all emojis from input tweets\n",
    "        emojis = extract_emojis(tweet)\n",
    "        E.append(emojis)\n",
    "\n",
    "        # save emojis into target array \n",
    "        for x in emojis:\n",
    "            if x in emoji_list:\n",
    "                ix = Edict[x]\n",
    "                target_arr[ii,ix] = 1\n",
    "\n",
    "        # save sanitized text\n",
    "        text = sanitize_tweets(tweet)\n",
    "        text = delete_emojis(text)\n",
    "        Text.append(text)\n",
    "        ii = ii+1\n",
    "\n",
    "    data = data + Text\n",
    "    rawdata = rawdata + RawText\n",
    "    raw_target = raw_target + E\n",
    "    filenames = filenames + [fullfile]\n",
    "    # search_emoji = search_emoji + [keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional now: get 1-D target\n",
    "numEmojis = len(target_names)\n",
    "arr = []\n",
    "for i in range(numEmojis):\n",
    "    arr.extend([i] * Len[i]) \n",
    "target_1d = np.array(arr, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2381, 3461, 3589, 2761, 3273, 3360, 2756, 2417, 3450, 3475, 3531, 3196, 3345, 3989, 3030, 3424, 3041, 3157, 3260, 3579, 2344, 3424, 3865, 1790, 4105, 3445, 3459, 3532, 2805, 1628, 2968, 3990, 3312, 3365, 3520, 3661, 3137, 3479, 3994, 3938, 3461, 3963, 3372, 3764, 2958, 2541, 3811, 3300, 3851, 3985, 2893, 3102, 2564, 2507, 2481, 3254, 3609, 3153, 3476, 1768, 1522, 2492, 3095, 2437, 3610, 3384, 3747, 1025, 3844, 3108, 3056, 2783, 2238, 3359, 3474]\n"
     ]
    }
   ],
   "source": [
    "print(Len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this struct is saving relatively raw data, for later use ~ visualizations\n",
    "class tweet_data:\n",
    "    pass\n",
    "\n",
    "D = tweet_data()\n",
    "D.raw_data = rawdata\n",
    "D.data = data\n",
    "D.raw_target = raw_target\n",
    "D.filenames = filenames\n",
    "D.numTweets = Len\n",
    "D.target_1d = target_1d\n",
    "D.target_arr = target_arr\n",
    "D.target_names = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/02\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_data_75xunique_target_arr.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(D, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_data_75xsingles.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(D, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-94a0bbfd8c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target_arr, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be directly loaded to train model\n",
    "class train_data:\n",
    "    pass\n",
    "\n",
    "T = train_data()\n",
    "T.data = data\n",
    "# X = vectorizer.fit_transform(data)\n",
    "# T.X = X\n",
    "# T.feature_names = vectorizer.get_feature_names()\n",
    "# T.X_shape = X.shape\n",
    "# T.Y = target_arr\n",
    "T.y = target_1d\n",
    "T.target_names = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'train_data_75xsingles.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(T, fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
