{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            \n",
    "            text = re.sub(\"|\".join(word), \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜˜', 'ğŸ˜—', 'ğŸ˜™', 'ğŸ˜š', 'â˜ºï¸', 'ğŸ™‚', 'ğŸ¤—', 'ğŸ¤”', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜¶', 'ğŸ™„', 'ğŸ˜', 'ğŸ˜£', 'ğŸ˜¥', 'ğŸ˜®', 'ğŸ¤', 'ğŸ˜¯', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ˜´', 'ğŸ˜Œ', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤¤', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ™ƒ', 'ğŸ¤‘', 'ğŸ˜²', 'â˜¹ï¸', 'ğŸ™', 'ğŸ˜–', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜¤', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜¬', 'ğŸ˜°', 'ğŸ˜±', 'ğŸ˜³', 'ğŸ˜µ', 'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ˜·', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ¤¢', 'ğŸ¤§', 'ğŸ˜‡', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¥', 'ğŸ¤“']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# load emoji list\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Emoji/\"+'mySmileys.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    emoji_list = pickle.load(fp)\n",
    "print(emoji_list)\n",
    "print(len(emoji_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sanitize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     and other words! '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize_tweets(tweet):\n",
    "    # delete twitter-specific tags\n",
    "    b1 = re.findall(r'@\\S*', tweet) # Twitter user names\n",
    "    b2 = re.findall(r'RT\\S*', tweet) # \"RT\"\n",
    "    b3 = re.findall(r'http\\S*', tweet) # links\n",
    "\n",
    "    c = tweet #list(tweet.split())\n",
    "    for b in b1:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b2:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    for b in b3:\n",
    "        c = re.sub(re.escape(b),\"\",c)\n",
    "    return c\n",
    "    \n",
    "tweet = \"RT @aa RT @bbb https:__ and other words! httpsaa\" #RawT[-1]\n",
    "sanitize_tweets(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜€\n",
      "ğŸ˜\n",
      "ğŸ˜‚\n",
      "ğŸ¤£\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜…\n",
      "ğŸ˜†\n",
      "ğŸ˜‰\n",
      "ğŸ˜Š\n",
      "ğŸ˜‹\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜˜\n",
      "ğŸ˜—\n",
      "ğŸ˜™\n",
      "ğŸ˜š\n",
      "â˜ºï¸\n",
      "ğŸ™‚\n",
      "ğŸ¤—\n",
      "ğŸ¤”\n",
      "ğŸ˜\n",
      "ğŸ˜‘\n",
      "ğŸ˜¶\n",
      "ğŸ™„\n",
      "ğŸ˜\n",
      "ğŸ˜£\n",
      "ğŸ˜¥\n",
      "ğŸ˜®\n",
      "ğŸ¤\n",
      "ğŸ˜¯\n",
      "ğŸ˜ª\n",
      "ğŸ˜«\n",
      "ğŸ˜´\n",
      "ğŸ˜Œ\n",
      "ğŸ˜›\n",
      "ğŸ˜œ\n",
      "ğŸ˜\n",
      "ğŸ¤¤\n",
      "ğŸ˜’\n",
      "ğŸ˜“\n",
      "ğŸ˜”\n",
      "ğŸ˜•\n",
      "ğŸ™ƒ\n",
      "ğŸ¤‘\n",
      "ğŸ˜²\n",
      "â˜¹ï¸\n",
      "ğŸ™\n",
      "ğŸ˜–\n",
      "ğŸ˜\n",
      "ğŸ˜Ÿ\n",
      "ğŸ˜¤\n",
      "ğŸ˜¢\n",
      "ğŸ˜­\n",
      "ğŸ˜¦\n",
      "ğŸ˜§\n",
      "ğŸ˜¨\n",
      "ğŸ˜©\n",
      "ğŸ˜¬\n",
      "ğŸ˜°\n",
      "ğŸ˜±\n",
      "ğŸ˜³\n",
      "ğŸ˜µ\n",
      "ğŸ˜¡\n",
      "ğŸ˜ \n",
      "ğŸ˜·\n",
      "ğŸ¤’\n",
      "ğŸ¤•\n",
      "ğŸ¤¢\n",
      "ğŸ¤§\n",
      "ğŸ˜‡\n",
      "ğŸ¤ \n",
      "ğŸ¤¡\n",
      "ğŸ¤¥\n",
      "ğŸ¤“\n"
     ]
    }
   ],
   "source": [
    "# preprocessing with removing repeat tweets\n",
    "# IN\n",
    "target_names = emoji_list # ['ğŸ˜','ğŸ˜¡'] # \n",
    "\n",
    "# OUT\n",
    "rawdata = []\n",
    "data = []\n",
    "raw_target = []\n",
    "Len = []\n",
    "filenames = []\n",
    "target_arr = np.zeros((5000*75*2, 75)) # init bigger than actual\n",
    "search_emoji = []\n",
    "\n",
    "# main loop\n",
    "ii = 0\n",
    "for keyword in target_names:\n",
    "    print(keyword)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets1 = pickle.load(fp)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets2 = pickle.load(fp)\n",
    "    Tweets = Tweets1+Tweets2\n",
    "\n",
    "    # remove repeat tweets\n",
    "    myset = set(Tweets)\n",
    "    Tweets_unique = list(myset)\n",
    "\n",
    "    Text = []\n",
    "    RawText = []\n",
    "    E = []\n",
    "    for tweet in Tweets_unique:\n",
    "        # remove tweets with more than one smiley\n",
    "#             emojis = extract_emojis(tweet)\n",
    "#             a = set(emojis) & set(emoji_list)\n",
    "#             a2 = list(a)\n",
    "#             if len(a2)==1 and a2[0]==keyword: # else: deal with later if doing multi-D target\n",
    "\n",
    "        # save raw tweet\n",
    "        RawText.append(tweet)\n",
    "        # save all emojis from input tweets\n",
    "        emojis = extract_emojis(tweet)\n",
    "        E.append(emojis)\n",
    "\n",
    "        # save emojis into target array \n",
    "        for x in emojis:\n",
    "            if x in emoji_list:\n",
    "                ix = Edict[x]\n",
    "                target_arr[ii,ix] = 1\n",
    "\n",
    "        # save sanitized text\n",
    "        text = sanitize_tweets(tweet)\n",
    "        text = delete_emojis(text)\n",
    "        Text.append(text)\n",
    "        ii = ii+1\n",
    "\n",
    "    data = data + Text\n",
    "    rawdata = rawdata + RawText\n",
    "    raw_target = raw_target + E\n",
    "    filenames = filenames + [fullfile]\n",
    "    search_emoji = search_emoji + [keyword]\n",
    "    Len.append(len(Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make 75-dim target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ğŸ˜€': 0, 'ğŸ˜': 1, 'ğŸ˜‚': 2, 'ğŸ¤£': 3, 'ğŸ˜ƒ': 4, 'ğŸ˜„': 5, 'ğŸ˜…': 6, 'ğŸ˜†': 7, 'ğŸ˜‰': 8, 'ğŸ˜Š': 9, 'ğŸ˜‹': 10, 'ğŸ˜': 11, 'ğŸ˜': 12, 'ğŸ˜˜': 13, 'ğŸ˜—': 14, 'ğŸ˜™': 15, 'ğŸ˜š': 16, 'â˜ºï¸': 17, 'ğŸ™‚': 18, 'ğŸ¤—': 19, 'ğŸ¤”': 20, 'ğŸ˜': 21, 'ğŸ˜‘': 22, 'ğŸ˜¶': 23, 'ğŸ™„': 24, 'ğŸ˜': 25, 'ğŸ˜£': 26, 'ğŸ˜¥': 27, 'ğŸ˜®': 28, 'ğŸ¤': 29, 'ğŸ˜¯': 30, 'ğŸ˜ª': 31, 'ğŸ˜«': 32, 'ğŸ˜´': 33, 'ğŸ˜Œ': 34, 'ğŸ˜›': 35, 'ğŸ˜œ': 36, 'ğŸ˜': 37, 'ğŸ¤¤': 38, 'ğŸ˜’': 39, 'ğŸ˜“': 40, 'ğŸ˜”': 41, 'ğŸ˜•': 42, 'ğŸ™ƒ': 43, 'ğŸ¤‘': 44, 'ğŸ˜²': 45, 'â˜¹ï¸': 46, 'ğŸ™': 47, 'ğŸ˜–': 48, 'ğŸ˜': 49, 'ğŸ˜Ÿ': 50, 'ğŸ˜¤': 51, 'ğŸ˜¢': 52, 'ğŸ˜­': 53, 'ğŸ˜¦': 54, 'ğŸ˜§': 55, 'ğŸ˜¨': 56, 'ğŸ˜©': 57, 'ğŸ˜¬': 58, 'ğŸ˜°': 59, 'ğŸ˜±': 60, 'ğŸ˜³': 61, 'ğŸ˜µ': 62, 'ğŸ˜¡': 63, 'ğŸ˜ ': 64, 'ğŸ˜·': 65, 'ğŸ¤’': 66, 'ğŸ¤•': 67, 'ğŸ¤¢': 68, 'ğŸ¤§': 69, 'ğŸ˜‡': 70, 'ğŸ¤ ': 71, 'ğŸ¤¡': 72, 'ğŸ¤¥': 73, 'ğŸ¤“': 74}\n"
     ]
    }
   ],
   "source": [
    "N = []\n",
    "for ii in range(75):\n",
    "    N.append(ii)\n",
    "Edict = dict(zip(emoji_list, N))\n",
    "print(Edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜€\n",
      "ğŸ˜\n",
      "ğŸ˜‚\n",
      "ğŸ¤£\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜„\n",
      "ğŸ˜…\n",
      "ğŸ˜†\n",
      "ğŸ˜‰\n",
      "ğŸ˜Š\n",
      "ğŸ˜‹\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜˜\n",
      "ğŸ˜—\n",
      "ğŸ˜™\n",
      "ğŸ˜š\n",
      "â˜ºï¸\n",
      "ğŸ™‚\n",
      "ğŸ¤—\n",
      "ğŸ¤”\n",
      "ğŸ˜\n",
      "ğŸ˜‘\n",
      "ğŸ˜¶\n",
      "ğŸ™„\n",
      "ğŸ˜\n",
      "ğŸ˜£\n",
      "ğŸ˜¥\n",
      "ğŸ˜®\n",
      "ğŸ¤\n",
      "ğŸ˜¯\n",
      "ğŸ˜ª\n",
      "ğŸ˜«\n",
      "ğŸ˜´\n",
      "ğŸ˜Œ\n",
      "ğŸ˜›\n",
      "ğŸ˜œ\n",
      "ğŸ˜\n",
      "ğŸ¤¤\n",
      "ğŸ˜’\n",
      "ğŸ˜“\n",
      "ğŸ˜”\n",
      "ğŸ˜•\n",
      "ğŸ™ƒ\n",
      "ğŸ¤‘\n",
      "ğŸ˜²\n",
      "â˜¹ï¸\n",
      "ğŸ™\n",
      "ğŸ˜–\n",
      "ğŸ˜\n",
      "ğŸ˜Ÿ\n",
      "ğŸ˜¤\n",
      "ğŸ˜¢\n",
      "ğŸ˜­\n",
      "ğŸ˜¦\n",
      "ğŸ˜§\n",
      "ğŸ˜¨\n",
      "ğŸ˜©\n",
      "ğŸ˜¬\n",
      "ğŸ˜°\n",
      "ğŸ˜±\n",
      "ğŸ˜³\n",
      "ğŸ˜µ\n",
      "ğŸ˜¡\n",
      "ğŸ˜ \n",
      "ğŸ˜·\n",
      "ğŸ¤’\n",
      "ğŸ¤•\n",
      "ğŸ¤¢\n",
      "ğŸ¤§\n",
      "ğŸ˜‡\n",
      "ğŸ¤ \n",
      "ğŸ¤¡\n",
      "ğŸ¤¥\n",
      "ğŸ¤“\n"
     ]
    }
   ],
   "source": [
    "# 75-D preprocessing\n",
    "# IN\n",
    "target_names = emoji_list \n",
    "\n",
    "# OUT\n",
    "rawdata = []\n",
    "data = []\n",
    "raw_target = []\n",
    "Len = []\n",
    "filenames = []\n",
    "target_arr = np.zeros((5000*75*2, 75))\n",
    "# search_emoji = []\n",
    "\n",
    "# main loop\n",
    "ii = 0\n",
    "for keyword in target_names:\n",
    "    print(keyword)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets1 = pickle.load(fp)\n",
    "    fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k_batch2/\"+'outfile'+keyword)\n",
    "    with open(fullfile, 'rb') as fp:\n",
    "        Tweets2 = pickle.load(fp)\n",
    "    Tweets = Tweets1 + Tweets2\n",
    "    \n",
    "    Len.append(len(Tweets))\n",
    "\n",
    "    Text = []\n",
    "    RawText = []\n",
    "    E = []\n",
    "    for tweet in Tweets:\n",
    "        # save raw tweet\n",
    "        RawText.append(tweet)\n",
    "        # take out all emojis from input tweets\n",
    "        emojis = extract_emojis(tweet)\n",
    "        E.append(emojis)\n",
    "\n",
    "        # save emojis into target array \n",
    "        for x in emojis:\n",
    "            if x in emoji_list:\n",
    "                ix = Edict[x]\n",
    "                target_arr[ii,ix] = 1\n",
    "\n",
    "        # save sanitized text\n",
    "        text = sanitize_tweets(tweet)\n",
    "        text = delete_emojis(text)\n",
    "        Text.append(text)\n",
    "        ii = ii+1\n",
    "\n",
    "    data = data + Text\n",
    "    rawdata = rawdata + RawText\n",
    "    raw_target = raw_target + E\n",
    "    filenames = filenames + [fullfile]\n",
    "    # search_emoji = search_emoji + [keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional now: get 1-D target\n",
    "numEmojis = len(target_names)\n",
    "arr = []\n",
    "for i in range(numEmojis):\n",
    "    arr.extend([i] * Len[i]) \n",
    "target_1d = np.array(arr, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2381, 3461, 3589, 2761, 3273, 3360, 2756, 2417, 3450, 3475, 3531, 3196, 3345, 3989, 3030, 3424, 3041, 3157, 3260, 3579, 2344, 3424, 3865, 1790, 4105, 3445, 3459, 3532, 2805, 1628, 2968, 3990, 3312, 3365, 3520, 3661, 3137, 3479, 3994, 3938, 3461, 3963, 3372, 3764, 2958, 2541, 3811, 3300, 3851, 3985, 2893, 3102, 2564, 2507, 2481, 3254, 3609, 3153, 3476, 1768, 1522, 2492, 3095, 2437, 3610, 3384, 3747, 1025, 3844, 3108, 3056, 2783, 2238, 3359, 3474]\n"
     ]
    }
   ],
   "source": [
    "print(Len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this struct is saving relatively raw data, for later use ~ visualizations\n",
    "class tweet_data:\n",
    "    pass\n",
    "\n",
    "D = tweet_data()\n",
    "D.raw_data = rawdata\n",
    "D.data = data\n",
    "D.raw_target = raw_target\n",
    "D.filenames = filenames\n",
    "D.numTweets = Len\n",
    "D.target_1d = target_1d\n",
    "D.target_arr = target_arr\n",
    "D.target_names = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/02\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_data_75xunique_target_arr.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(D, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_data_75xsingles.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(D, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-94a0bbfd8c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target_arr, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be directly loaded to train model\n",
    "class train_data:\n",
    "    pass\n",
    "\n",
    "T = train_data()\n",
    "T.data = data\n",
    "# X = vectorizer.fit_transform(data)\n",
    "# T.X = X\n",
    "# T.feature_names = vectorizer.get_feature_names()\n",
    "# T.X_shape = X.shape\n",
    "# T.Y = target_arr\n",
    "T.y = target_1d\n",
    "T.target_names = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'train_data_75xsingles.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(T, fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
