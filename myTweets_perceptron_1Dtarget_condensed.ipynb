{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😀', '😁', '😂', '🤣', '😃', '😄', '😅', '😆', '😉', '😊', '😋', '😎', '😍', '😘', '😗', '😙', '😚', '☺️', '🙂', '🤗', '🤔', '😐', '😑', '😶', '🙄', '😏', '😣', '😥', '😮', '🤐', '😯', '😪', '😫', '😴', '😌', '😛', '😜', '😝', '🤤', '😒', '😓', '😔', '😕', '🙃', '🤑', '😲', '☹️', '🙁', '😖', '😞', '😟', '😤', '😢', '😭', '😦', '😧', '😨', '😩', '😬', '😰', '😱', '😳', '😵', '😡', '😠', '😷', '🤒', '🤕', '🤢', '🤧', '😇', '🤠', '🤡', '🤥', '🤓']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# load emoji list\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Emoji/\"+'mySmileys.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    L = pickle.load(fp)\n",
    "print(L)\n",
    "print(len(L))\n",
    "target_names = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_data:\n",
    "    pass\n",
    "\n",
    "# D = tweet_data()\n",
    "# D.raw_data = rawdata\n",
    "# D.data = data\n",
    "# D.raw_target = elist\n",
    "# D.filesnames = filenames\n",
    "# D.numTweets = Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweets_75x5k.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    D = pickle.load(fp)\n",
    "\n",
    "len(D.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select subset\n",
    "target_names = ['😍','😡']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D.raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375000,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make 1D target based only on the single search-emoji\n",
    "# Len = []\n",
    "# for keyword in target_names:\n",
    "#     fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/5k/\"+'outfile'+keyword+'.p')\n",
    "#     with open(fullfile, 'rb') as fp:\n",
    "#         itemlist = pickle.load(fp)\n",
    "#         Len.append(len(itemlist))\n",
    "    \n",
    "# numTweets = 5000\n",
    "numEmojis = len(target_names)\n",
    "arr = []\n",
    "for i in range(numEmojis):\n",
    "    arr.extend([i] * D.numTweets[i]) \n",
    "target = np.array(arr, dtype=int)\n",
    "\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Tweet dataset; split training/testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is like a struct\n",
    "class tweet_train:\n",
    "    pass\n",
    "\n",
    "T = tweet_train()\n",
    "T.target_names = target_names\n",
    "T.data = D.data\n",
    "T.filenames = D.filesnames\n",
    "T.target = target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfile = os.path.expanduser(\"~/Dropbox/insight/Twitter/\"+'tweet_train_2x5k.p')\n",
    "with open(fullfile, 'rb') as fp:\n",
    "    T = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😀\n"
     ]
    }
   ],
   "source": [
    "T.target_names = target_names\n",
    "print(T.target_names[T.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    T.data, T.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier 2: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char',\n",
    "#                              use_idf=False)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word',\n",
    "                             use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "# TASK: Build a vectorizer / classifier pipeline using the previous analyzer\n",
    "# the pipeline instance should stored in a variable named clf\n",
    "clf = Pipeline([\n",
    "    ('vec', vectorizer),\n",
    "    ('clf', Perceptron(tol=1e-3)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ..._jobs=1, penalty=None, random_state=0,\n",
       "      shuffle=True, tol=0.001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: Fit the pipeline on the training set\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          😀       0.48      0.63      0.55      1006\n",
      "          😁       0.56      0.31      0.40      1018\n",
      "          😂       0.33      0.41      0.37      1004\n",
      "          🤣       0.69      0.49      0.57       975\n",
      "          😃       0.77      0.40      0.53      1003\n",
      "          😄       0.38      0.42      0.40      1012\n",
      "          😅       0.50      0.48      0.49      1005\n",
      "          😆       0.48      0.54      0.51      1022\n",
      "          😉       0.36      0.39      0.38      1041\n",
      "          😊       0.41      0.45      0.43       978\n",
      "          😋       0.63      0.46      0.53       975\n",
      "          😎       0.35      0.53      0.42      1040\n",
      "          😍       0.65      0.43      0.52      1004\n",
      "          😘       0.37      0.42      0.39      1007\n",
      "          😗       0.60      0.52      0.56      1001\n",
      "          😙       0.30      0.51      0.37       978\n",
      "          😚       0.66      0.52      0.58      1016\n",
      "         ☺️       0.51      0.48      0.50       994\n",
      "          🙂       0.44      0.46      0.45      1009\n",
      "          🤗       0.65      0.43      0.52      1024\n",
      "          🤔       0.28      0.60      0.38       983\n",
      "          😐       0.35      0.43      0.39      1005\n",
      "          😑       0.20      0.37      0.26       998\n",
      "          😶       0.74      0.68      0.71       989\n",
      "          🙄       0.40      0.28      0.33      1040\n",
      "          😏       0.55      0.40      0.47       974\n",
      "          😣       0.49      0.39      0.44       981\n",
      "          😥       0.41      0.37      0.39      1024\n",
      "          😮       0.55      0.56      0.55      1018\n",
      "          🤐       0.68      0.77      0.72       991\n",
      "          😯       0.46      0.50      0.48       993\n",
      "          😪       0.27      0.34      0.30       983\n",
      "          😫       0.36      0.46      0.40      1006\n",
      "          😴       0.43      0.56      0.49       986\n",
      "          😌       0.40      0.43      0.41      1010\n",
      "          😛       0.66      0.37      0.47      1008\n",
      "          😜       0.80      0.47      0.59      1027\n",
      "          😝       0.19      0.46      0.27      1020\n",
      "          🤤       0.53      0.40      0.46      1011\n",
      "          😒       0.31      0.33      0.32       991\n",
      "          😓       0.60      0.38      0.47      1016\n",
      "          😔       0.41      0.30      0.35       951\n",
      "          😕       0.44      0.43      0.44       949\n",
      "          🙃       0.29      0.35      0.32       998\n",
      "          🤑       0.69      0.63      0.66      1009\n",
      "          😲       0.76      0.60      0.67      1028\n",
      "         ☹️       0.42      0.31      0.36       966\n",
      "          🙁       0.58      0.45      0.51      1019\n",
      "          😖       0.18      0.36      0.24       946\n",
      "          😞       0.44      0.30      0.36      1027\n",
      "          😟       0.66      0.42      0.52       984\n",
      "          😤       0.69      0.47      0.56       975\n",
      "          😢       0.57      0.56      0.56       982\n",
      "          😭       0.61      0.57      0.59       981\n",
      "          😦       0.51      0.47      0.49      1072\n",
      "          😧       0.56      0.43      0.49       974\n",
      "          😨       0.49      0.39      0.43       989\n",
      "          😩       0.56      0.45      0.50       980\n",
      "          😬       0.47      0.40      0.43       987\n",
      "          😰       0.67      0.72      0.69      1009\n",
      "          😱       0.76      0.75      0.76       973\n",
      "          😳       0.75      0.56      0.64       980\n",
      "          😵       0.65      0.48      0.55      1025\n",
      "          😡       0.66      0.59      0.62       998\n",
      "          😠       0.38      0.39      0.39       966\n",
      "          😷       0.62      0.40      0.49      1026\n",
      "          🤒       0.57      0.50      0.53      1040\n",
      "          🤕       0.79      0.83      0.81      1039\n",
      "          🤢       0.46      0.41      0.43       964\n",
      "          🤧       0.53      0.50      0.52      1004\n",
      "          😇       0.73      0.57      0.64       978\n",
      "          🤠       0.70      0.61      0.65       995\n",
      "          🤡       0.40      0.70      0.51       978\n",
      "          🤥       0.47      0.64      0.54      1047\n",
      "          🤓       0.47      0.52      0.49       995\n",
      "\n",
      "avg / total       0.52      0.48      0.49     75000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=T.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4428c3902506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# poo-mark came from emojipedia:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# https://emojipedia-us.s3.amazonaws.com/thumbs/120/apple/96/pile-of-poo_1f4a9.png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpoo_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"poo-mark.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# from os.path import join, dirname, abspath\n",
    "# from matplotlib import pyplot\n",
    "# from matplotlib.cbook import get_sample_data\n",
    "# from numpy import linspace\n",
    "# from numpy.core.umath import pi\n",
    "# from numpy.ma import sin\n",
    "# # poo-mark came from emojipedia:\n",
    "# # https://emojipedia-us.s3.amazonaws.com/thumbs/120/apple/96/pile-of-poo_1f4a9.png\n",
    "# poo_img = pyplot.imread(get_sample_data(join(dirname(abspath(__file__)), \"poo-mark.png\")))\n",
    "# x = linspace(0, 2*pi, num=10)\n",
    "# y = sin(x)\n",
    "# fig, ax = pyplot.subplots()\n",
    "# plot = ax.plot(x, y, linestyle=\"-\")\n",
    "# ax_width = ax.get_window_extent().width\n",
    "# fig_width = fig.get_window_extent().width\n",
    "# fig_height = fig.get_window_extent().height\n",
    "# poo_size = ax_width/(fig_width*len(x))\n",
    "# poo_axs = [None for i in range(len(x))]\n",
    "# for i in range(len(x)):\n",
    "#     loc = ax.transData.transform((x[i], y[i]))\n",
    "#     poo_axs[i] = fig.add_axes([loc[0]/fig_width-poo_size/2, loc[1]/fig_height-poo_size/2,\n",
    "#                                poo_size, poo_size], anchor='C')\n",
    "#     poo_axs[i].imshow(poo_img)\n",
    "#     poo_axs[i].axis(\"off\")\n",
    "# fig.savefig(\"poo_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8925"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n",
    "\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall) \n",
    "https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/metrics/classification.py#L1363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[877  97]\n",
      " [131 895]]\n"
     ]
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm, cmap=plt.cm.jet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    u'lovely definition: 1. pleasant or enjoyable: 2. beautiful:',\n",
    "    u'Hate speech is a communication that carries no meaning other than the expression of hatred for some group',\n",
    "    u'amazing wow love this!!!',\n",
    "]\n",
    "predicted = clf.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😍', '😡', '😍']\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for ii in predicted:\n",
    "    output.append(T.target_names[ii])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "fullfile = os.path.expanduser(\"~/Dropbox/insight_datadir/Webapp_data/\"+'clf_0930.p')\n",
    "with open(fullfile, 'wb') as fp:\n",
    "    pickle.dump(clf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argsort(classifier.predict_proba(X), axis=1)[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron (1 hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.], [-1., -2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[coef.shape for coef in clf.coefs_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.53025854]\n",
      " [-0.86285329]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coefs_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.96718015e-004, 9.99803282e-001],\n",
       "       [1.00000000e+000, 4.67017947e-144]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba([[2., 2.], [-1., -2.]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 107790)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XV_test = vectorizer.transform(X_test)\n",
    "XV_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-94b2630ba532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n\u001b[0;32m--> 963\u001b[0;31m                                      key=itemgetter(1))]\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'what have you done!','Oh yeah? don''t try to trick me']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf1.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, T.target_names[category]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 283,
   "position": {
    "height": "40px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "326px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
