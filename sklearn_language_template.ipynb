{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build a language detector model\n",
    "\n",
    "The goal of this exercise is to train a linear classifier on text features\n",
    "that represent sequences of up to 3 consecutive characters so as to be\n",
    "recognize natural languages by using the frequencies of short character\n",
    "sequences as 'fingerprints'.\n",
    "\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/xiuye/anaconda3/envs/insight/lib/python3.6/site-packages/ipykernel_launcher.py', '-f', '/Users/xiuye/Library/Jupyter/runtime/kernel-862f9687-8d8b-4f2f-8dbc-831d6e2ad3a0.json']\n"
     ]
    }
   ],
   "source": [
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Dropbox/insight_side/20news-bydate-train/rec.autos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-84f52a1b0eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The training data folder must be passed as first argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlanguages_data_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'~/Dropbox/insight_side/20news-bydate-train/rec.autos'\u001b[0m\u001b[0;31m#sys.argv[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages_data_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.6/site-packages/sklearn/datasets/base.py\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     folders = [f for f in sorted(listdir(container_path))\n\u001b[0m\u001b[1;32m    167\u001b[0m                if isdir(join(container_path, f))]\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Dropbox/insight_side/20news-bydate-train/rec.autos'"
     ]
    }
   ],
   "source": [
    "# The training data folder must be passed as first argument\n",
    "languages_data_folder = sys.argv[1]\n",
    "dataset = load_files(languages_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char',\n",
    "                             use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a vectorizer / classifier pipeline using the previous analyzer\n",
    "# the pipeline instance should stored in a variable named clf\n",
    "clf = Pipeline([\n",
    "    ('vec', vectorizer),\n",
    "    ('clf', Perceptron(tol=1e-3)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Fit the pipeline on the training set\n",
    "clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matlotlib.pyplot as plt\n",
    "#plt.matshow(cm, cmap=plt.cm.jet)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    u'This is a language detection test.',\n",
    "    u'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    u'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "]\n",
    "predicted = clf.predict(sentences)\n",
    "\n",
    "for s, p in zip(sentences, predicted):\n",
    "    print(u'The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
